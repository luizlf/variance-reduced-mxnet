{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import imp\n",
    "import logging\n",
    "imp.reload(logging)\n",
    "logging.basicConfig(filename=r'C:\\Users\\luiz\\Projects\\vradam-mxnet\\example.log',filemode='w',level=logging.DEBUG)\n",
    "logging.debug('This message should go to the log file')\n",
    "import mxnet as mx\n",
    "import requests as rq\n",
    "import pandas as pd\n",
    "import time, datetime\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_svrg_module():\n",
    "\n",
    "    try:\n",
    "        os.makedirs('./svrg_optimization')\n",
    "        init_svrg = rq.get(\n",
    "            'https://raw.githubusercontent.com/apache/incubator-mxnet/master/python/mxnet/contrib/svrg_optimization/__init__.py')\n",
    "        with open('./svrg_optimization/__init__.py', 'w') as file_init:\n",
    "            file_init.write(init_svrg.text)\n",
    "\n",
    "        modu_svrg = rq.get(\n",
    "            'https://raw.githubusercontent.com/apache/incubator-mxnet/master/python/mxnet/contrib/svrg_optimization/svrg_module.py')\n",
    "        with open('./svrg_optimization/svrg_module.py', 'w') as file_modu:\n",
    "            file_modu.write(modu_svrg.text)\n",
    "\n",
    "        opti_svrg = rq.get(\n",
    "            'https://raw.githubusercontent.com/apache/incubator-mxnet/master/python/mxnet/contrib/svrg_optimization/svrg_optimizer.py')\n",
    "        with open('./svrg_optimization/svrg_optimizer.py', 'w') as file_opti:\n",
    "            file_opti.write(opti_svrg.text)\n",
    "\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from svrg_optimization.svrg_module import SVRGModule\n",
    "except ModuleNotFoundError:\n",
    "    download_svrg_module()\n",
    "    from svrg_optimization.svrg_module import SVRGModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "\n",
    "    def __init__(self, dataset, epochs, context, batch_size, is_svrg=True, optimizer='sgd', hyperparams=None, **kwargs):\n",
    "        self.dataset = dataset.lower()\n",
    "        self.epochs = epochs\n",
    "        self.context = context\n",
    "        self.hyperparams = hyperparams\n",
    "        self.batch_size = batch_size\n",
    "        self.is_svrg = is_svrg\n",
    "        self.optimizer = optimizer\n",
    "        self.train_iter = None\n",
    "        self.test_iter = None\n",
    "\n",
    "        self.define_dataset()\n",
    "        self.log_path = r'evals.log'\n",
    "        import logging\n",
    "        imp.reload(logging)\n",
    "        logging.basicConfig(filename=self.log_path,filemode='w',level=logging.DEBUG)\n",
    "        #logging.debug('This message should go to the log file')\n",
    "        #self.logger = logging.getLogger()\n",
    "        #self.logger.setLevel(logging.DEBUG)  # logging to stdout\n",
    "\n",
    "        # return super().__init__(dataset, epochs, context, hyperparams, **kwargs)\n",
    "\n",
    "    def define_dataset(self):\n",
    "        if self.dataset == 'mnist' or self.dataset is None:\n",
    "            # define dataset and dataloader\n",
    "            mnist = mx.test_utils.get_mnist()\n",
    "\n",
    "            self.train_iter = mx.io.NDArrayIter(mnist['train_data'],\n",
    "                                                mnist['train_label'],\n",
    "                                                self.batch_size, shuffle=True, data_name='data',\n",
    "                                                label_name='softmax_label')\n",
    "            self.test_iter = mx.io.NDArrayIter(mnist['test_data'],\n",
    "                                               mnist['test_label'],\n",
    "                                               self.batch_size * 2, shuffle=False, data_name='data',\n",
    "                                               label_name='softmax_label')\n",
    "\n",
    "        elif self.dataset == 'cifar-10':\n",
    "            mx.test_utils.get_cifar10()\n",
    "\n",
    "    def process_log(self):\n",
    "        # define the paths to the training logs\n",
    "        logs = [\n",
    "            (0, self.log_path)\n",
    "        ]\n",
    "\n",
    "        # initialize the list of train rank-1 and rank-5 accuracies, along\n",
    "        # with the training loss\n",
    "        (trainRank1, trainRank5, trainLoss) = ([], [], [])\n",
    "\n",
    "        # initialize the list of validation rank-1 and rank-5 accuracies,\n",
    "        # along with the validation loss\n",
    "        (valRank1, valRank5, valLoss) = ([], [], [])\n",
    "\n",
    "        # loop over the training logs\n",
    "        for (i, (endEpoch, p)) in enumerate(logs):\n",
    "            # load the contents of the log file, then initialize the batch\n",
    "            # lists for the training and validation data\n",
    "            rows = open(p).read().strip()\n",
    "            (bTrainRank1, bTrainRank5, bTrainLoss) = ([], [], [])\n",
    "            (bValRank1, bValRank5, bValLoss) = ([], [], [])\n",
    "\n",
    "            # grab the set of training epochs\n",
    "            epochs = set(re.findall(r'Epoch\\[(\\d+)\\]', rows))\n",
    "            epochs = sorted([int(e) for e in epochs])\n",
    "\n",
    "            # loop over the epochs\n",
    "            for e in epochs:\n",
    "                # find all rank-1 accuracies, rank-5 accuracies, and loss\n",
    "                # values, then take the final entry in the list for each\n",
    "                s = r'Epoch\\[' + str(e) + '\\].*accuracy=([0]*\\.?[0-9]+)'\n",
    "\n",
    "                rank1 = re.findall(s, rows)[-2]\n",
    "                #print(rank1)\n",
    "                #s = r'Epoch\\[' + str(e) + '\\].*top_k_accuracy_5=([0]*\\.?[0-9]+)'\n",
    "                #rank5 = re.findall(s, rows)[-2]\n",
    "                s = r'Epoch\\[' + str(e) + '\\].*cross-entropy=([0-9]*\\.?[0-9]+)'\n",
    "                loss = re.findall(s, rows)[-2]\n",
    "\n",
    "                # update the batch training lists\n",
    "                bTrainRank1.append(float(rank1))\n",
    "                #bTrainRank5.append(float(rank5))\n",
    "                bTrainLoss.append(float(loss))\n",
    "\n",
    "            # extract the validation rank-1 and rank-5 accuracies for each\n",
    "            # epoch, followed by the loss\n",
    "            bValRank1 = re.findall(r'Validation-accuracy=(.*)', rows)\n",
    "            #bValRank5 = re.findall(r'Validation-top_k_accuracy_5=(.*)', rows)\n",
    "            bValLoss = re.findall(r'Validation-cross-entropy=(.*)', rows)\n",
    "\n",
    "            # convert the validation rank-1, rank-5, and loss lists to floats\n",
    "            bValRank1 = [float(x) for x in bValRank1]\n",
    "            #bValRank5 = [float(x) for x in bValRank5]\n",
    "            bValLoss = [float(x) for x in bValLoss]\n",
    "            \n",
    "            df = pd.DataFrame({'val_acc': bValRank1,\n",
    "                               'val_loss': bValLoss,\n",
    "                               'train_acc': bTrainRank1,\n",
    "                               'train_loss': bTrainLoss})\n",
    "            \n",
    "            return df\n",
    "            \n",
    "    \n",
    "    def run_logistic(self, update_freq=2):\n",
    "\n",
    "        # data input and formatting\n",
    "        data = mx.sym.var('data')  # (bs, 1, 28, 28) - MNIST\n",
    "        label = mx.sym.var('softmax_label')\n",
    "        data = mx.sym.Flatten(data)  # (bs, 28*28) - MNIST\n",
    "\n",
    "        # logistic regression network\n",
    "        fc = mx.sym.FullyConnected(data, num_hidden=10, name='fc')\n",
    "        logist = mx.sym.SoftmaxOutput(fc, label=label, name='softmax')\n",
    "    \n",
    "        # metrics and eval\n",
    "        metric_list = [mx.metric.Accuracy(output_names=['softmax_output'], label_names=['softmax_label']),\n",
    "                       mx.metric.CrossEntropy(output_names=['softmax_output'], label_names=['softmax_label'])]\n",
    "        eval_metrics = mx.metric.CompositeEvalMetric(metric_list)\n",
    "\n",
    "        # create and 'compile' network\n",
    "        if self.is_svrg:\n",
    "            model = SVRGModule(symbol=logist,\n",
    "                               data_names=['data'],\n",
    "                               label_names=['softmax_label'],\n",
    "                               context=self.context,\n",
    "                               update_freq=update_freq)\n",
    "        else:\n",
    "            model = mx.mod.Module(logist,\n",
    "                                  data_names=['data'],\n",
    "                                  label_names=['softmax_label'],\n",
    "                                  context=self.context)\n",
    "\n",
    "        model.bind(data_shapes=self.train_iter.provide_data,\n",
    "                   label_shapes=self.train_iter.provide_label)\n",
    "        model.init_params()\n",
    "        model.init_optimizer(kvstore='local',\n",
    "                             optimizer=self.optimizer,\n",
    "                             optimizer_params=self.hyperparams)\n",
    "\n",
    "        timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        model.fit(self.train_iter,  # train data\n",
    "                  eval_data=self.test_iter,  # validation data\n",
    "                  # optimizer='sgd',  # use SGD to train\n",
    "                  # optimizer_params = (('learning_rate', lr),),  # use fixed learning rate\n",
    "                  eval_metric=eval_metrics,  # report accuracy during training\n",
    "                  # output progress for each 500 data batches\n",
    "                  batch_end_callback=mx.callback.Speedometer(self.batch_size, 1000),\n",
    "                  #epoch_end_callback=mx.callback.log_train_metric, #self.log_call,#mx.callback.do_checkpoint('logistic', 10),\n",
    "                  num_epoch=self.epochs)  # train for at most <epochs> dataset passes\n",
    "        \n",
    "        self.df_out = self.process_log()\n",
    "        print(timestamp)\n",
    "        self.df_out['timestamp'] = pd.Series(timestamp, index=self.df_out.index)\n",
    "        self.df_out['optimizer'] = pd.Series(self.optimizer, index=self.df_out.index)\n",
    "        self.df_out['is_svrg'] = pd.Series(self.is_svrg, index=self.df_out.index)\n",
    "        self.df_out['batch_size'] = pd.Series(self.batch_size, index=self.df_out.index)\n",
    "        self.df_out['hyperparams'] = pd.Series(str(self.hyperparams), index=self.df_out.index)\n",
    "        self.df_out['epoch'] = pd.Series(self.df_out.index + 1, index=self.df_out.index)\n",
    "        self.df_out['update_freq'] = pd.Series(update_freq, index=self.df_out.index)\n",
    "        \n",
    "        #print(self.df_out)\n",
    "        #val_score = model.score(self.train_iter, 'acc')[0][1]\n",
    "    \n",
    "        #train_score = model.score(self.train_iter, 'acc')[0][1]\n",
    "        #print(val_score, train_score)\n",
    "\n",
    "        # TODO: Define function returns\n",
    "        # IDEA: pandas df with all class params, timestamp, and scores. Save this to disk\n",
    "        # lr_params_svrg.update({lr: [val_score, train_score]})\n",
    "\n",
    "        #return train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "\n",
    "    def __init__(self, dataset, epochs, context, batch_size, is_svrg=True, optimizer='sgd', hyperparams=None, **kwargs):\n",
    "        self.dataset = dataset.lower()\n",
    "        self.epochs = epochs\n",
    "        self.context = context\n",
    "        self.hyperparams = hyperparams\n",
    "        self.batch_size = batch_size\n",
    "        self.is_svrg = is_svrg\n",
    "        self.optimizer = optimizer\n",
    "        self.train_iter = None\n",
    "        self.test_iter = None\n",
    "\n",
    "        self.define_dataset()\n",
    "        self.log_path = r'evals.log'\n",
    "        import logging\n",
    "        imp.reload(logging)\n",
    "        logging.basicConfig(filename=self.log_path,filemode='w',level=logging.DEBUG)\n",
    "        #logging.debug('This message should go to the log file')\n",
    "        #self.logger = logging.getLogger()\n",
    "        #self.logger.setLevel(logging.DEBUG)  # logging to stdout\n",
    "\n",
    "        # return super().__init__(dataset, epochs, context, hyperparams, **kwargs)\n",
    "\n",
    "    def define_dataset(self):\n",
    "        if self.dataset == 'mnist' or self.dataset is None:\n",
    "            # define dataset and dataloader\n",
    "            mnist = mx.test_utils.get_mnist()\n",
    "\n",
    "            self.train_iter = mx.io.NDArrayIter(mnist['train_data'],\n",
    "                                                mnist['train_label'],\n",
    "                                                self.batch_size, shuffle=True, data_name='data',\n",
    "                                                label_name='softmax_label')\n",
    "            self.test_iter = mx.io.NDArrayIter(mnist['test_data'],\n",
    "                                               mnist['test_label'],\n",
    "                                               self.batch_size * 2, shuffle=False, data_name='data',\n",
    "                                               label_name='softmax_label')\n",
    "\n",
    "        elif self.dataset == 'cifar-10':\n",
    "            mx.test_utils.get_cifar10()\n",
    "\n",
    "    def process_log(self):\n",
    "        # define the paths to the training logs\n",
    "        logs = [\n",
    "            (0, self.log_path)\n",
    "        ]\n",
    "\n",
    "        # initialize the list of train rank-1 and rank-5 accuracies, along\n",
    "        # with the training loss\n",
    "        (trainRank1, trainRank5, trainLoss) = ([], [], [])\n",
    "\n",
    "        # initialize the list of validation rank-1 and rank-5 accuracies,\n",
    "        # along with the validation loss\n",
    "        (valRank1, valRank5, valLoss) = ([], [], [])\n",
    "\n",
    "        # loop over the training logs\n",
    "        for (i, (endEpoch, p)) in enumerate(logs):\n",
    "            # load the contents of the log file, then initialize the batch\n",
    "            # lists for the training and validation data\n",
    "            rows = open(p).read().strip()\n",
    "            (bTrainRank1, bTrainRank5, bTrainLoss) = ([], [], [])\n",
    "            (bValRank1, bValRank5, bValLoss) = ([], [], [])\n",
    "\n",
    "            # grab the set of training epochs\n",
    "            epochs = set(re.findall(r'Epoch\\[(\\d+)\\]', rows))\n",
    "            epochs = sorted([int(e) for e in epochs])\n",
    "\n",
    "            # loop over the epochs\n",
    "            for e in epochs:\n",
    "                # find all rank-1 accuracies, rank-5 accuracies, and loss\n",
    "                # values, then take the final entry in the list for each\n",
    "                s = r'Epoch\\[' + str(e) + '\\].*accuracy=([0]*\\.?[0-9]+)'\n",
    "\n",
    "                rank1 = re.findall(s, rows)[-2]\n",
    "                #print(rank1)\n",
    "                #s = r'Epoch\\[' + str(e) + '\\].*top_k_accuracy_5=([0]*\\.?[0-9]+)'\n",
    "                #rank5 = re.findall(s, rows)[-2]\n",
    "                s = r'Epoch\\[' + str(e) + '\\].*cross-entropy=([0-9]*\\.?[0-9]+)'\n",
    "                loss = re.findall(s, rows)[-2]\n",
    "\n",
    "                # update the batch training lists\n",
    "                bTrainRank1.append(float(rank1))\n",
    "                #bTrainRank5.append(float(rank5))\n",
    "                bTrainLoss.append(float(loss))\n",
    "\n",
    "            # extract the validation rank-1 and rank-5 accuracies for each\n",
    "            # epoch, followed by the loss\n",
    "            bValRank1 = re.findall(r'Validation-accuracy=(.*)', rows)\n",
    "            #bValRank5 = re.findall(r'Validation-top_k_accuracy_5=(.*)', rows)\n",
    "            bValLoss = re.findall(r'Validation-cross-entropy=(.*)', rows)\n",
    "\n",
    "            # convert the validation rank-1, rank-5, and loss lists to floats\n",
    "            bValRank1 = [float(x) for x in bValRank1]\n",
    "            #bValRank5 = [float(x) for x in bValRank5]\n",
    "            bValLoss = [float(x) for x in bValLoss]\n",
    "            \n",
    "            df = pd.DataFrame({'val_acc': bValRank1,\n",
    "                               'val_loss': bValLoss,\n",
    "                               'train_acc': bTrainRank1,\n",
    "                               'train_loss': bTrainLoss})\n",
    "            \n",
    "            return df\n",
    "            \n",
    "    \n",
    "    def run_neural(self, update_freq=2, act='relu'):\n",
    "\n",
    "        # data input and formatting\n",
    "        data = mx.sym.var('data')  # (bs, 1, 28, 28) - MNIST\n",
    "        label = mx.sym.var('softmax_label')\n",
    "        data = mx.sym.Flatten(data)  # (bs, 28*28) - MNIST\n",
    "\n",
    "        # neural network\n",
    "        net = mx.sym.FullyConnected(data, num_hidden=10, name='fc1')\n",
    "        net = mx.sym.Activation(data=net, act_type=act, name='ReLU1')\n",
    "        net = mx.sym.FullyConnected(net, num_hidden=1000, name='fc2')\n",
    "        net = mx.sym.Activation(data=net, act_type=act, name='ReLU2')\n",
    "        net = mx.sym.FullyConnected(net, num_hidden=1000, name='fc3')\n",
    "        net = mx.sym.Activation(data=net, act_type=act, name='ReLU3')\n",
    "        net = mx.sym.SoftmaxOutput(net, label=label, name='softmax')\n",
    "    \n",
    "        # metrics and eval\n",
    "        metric_list = [mx.metric.Accuracy(output_names=['softmax_output'], label_names=['softmax_label']),\n",
    "                       mx.metric.CrossEntropy(output_names=['softmax_output'], label_names=['softmax_label'])]\n",
    "        eval_metrics = mx.metric.CompositeEvalMetric(metric_list)\n",
    "\n",
    "        # create and 'compile' network\n",
    "        if self.is_svrg:\n",
    "            model = SVRGModule(symbol=net,\n",
    "                               data_names=['data'],\n",
    "                               label_names=['softmax_label'],\n",
    "                               context=self.context,\n",
    "                               update_freq=update_freq)\n",
    "        else:\n",
    "            model = mx.mod.Module(net,\n",
    "                                  data_names=['data'],\n",
    "                                  label_names=['softmax_label'],\n",
    "                                  context=self.context)\n",
    "\n",
    "        model.bind(data_shapes=self.train_iter.provide_data,\n",
    "                   label_shapes=self.train_iter.provide_label)\n",
    "        model.init_params()\n",
    "        model.init_optimizer(kvstore='local',\n",
    "                             optimizer=self.optimizer,\n",
    "                             optimizer_params=self.hyperparams)\n",
    "\n",
    "        timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        model.fit(self.train_iter,  # train data\n",
    "                  eval_data=self.test_iter,  # validation data\n",
    "                  # optimizer='sgd',  # use SGD to train\n",
    "                  # optimizer_params = (('learning_rate', lr),),  # use fixed learning rate\n",
    "                  eval_metric=eval_metrics,  # report accuracy during training\n",
    "                  # output progress for each 500 data batches\n",
    "                  batch_end_callback=mx.callback.Speedometer(self.batch_size, 1000),\n",
    "                  #epoch_end_callback=mx.callback.log_train_metric, #self.log_call,#mx.callback.do_checkpoint('logistic', 10),\n",
    "                  num_epoch=self.epochs)  # train for at most <epochs> dataset passes\n",
    "        \n",
    "        self.df_out = self.process_log()\n",
    "        print(timestamp)\n",
    "        self.df_out['timestamp'] = pd.Series(timestamp, index=self.df_out.index)\n",
    "        self.df_out['optimizer'] = pd.Series(self.optimizer, index=self.df_out.index)\n",
    "        self.df_out['is_svrg'] = pd.Series(self.is_svrg, index=self.df_out.index)\n",
    "        self.df_out['batch_size'] = pd.Series(self.batch_size, index=self.df_out.index)\n",
    "        self.df_out['hyperparams'] = pd.Series(str(self.hyperparams), index=self.df_out.index)\n",
    "        self.df_out['epoch'] = pd.Series(self.df_out.index + 1, index=self.df_out.index)\n",
    "        self.df_out['update_freq'] = pd.Series(update_freq, index=self.df_out.index)\n",
    "        \n",
    "        #print(self.df_out)\n",
    "        #val_score = model.score(self.train_iter, 'acc')[0][1]\n",
    "    \n",
    "        #train_score = model.score(self.train_iter, 'acc')[0][1]\n",
    "        #print(val_score, train_score)\n",
    "\n",
    "        # TODO: Define function returns\n",
    "        # IDEA: pandas df with all class params, timestamp, and scores. Save this to disk\n",
    "        # lr_params_svrg.update({lr: [val_score, train_score]})\n",
    "\n",
    "        #return train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1544064628.7675602\n",
      "2018-12-06 00:50:28\n",
      "1544064637.6647947\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(dataset='MNIST', \n",
    "                   optimizer='sgd', \n",
    "                   epochs=10, \n",
    "                   context=mx.gpu(), \n",
    "                   batch_size=128, \n",
    "                   is_svrg=False, \n",
    "                   hyperparams={'learning_rate': 0.3})\n",
    "\n",
    "print(time.time())\n",
    "nn.run_neural(1, act='relu')\n",
    "print(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6s epoch - cpu False\n",
    "# 100s epoch - cpu True\n",
    "# 107s epoch - gpu True\n",
    "# .8s epoch - gpu False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>is_svrg</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>epoch</th>\n",
       "      <th>update_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.876172</td>\n",
       "      <td>0.414475</td>\n",
       "      <td>0.491571</td>\n",
       "      <td>1.864575</td>\n",
       "      <td>2018-12-06 00:50:28</td>\n",
       "      <td>sgd</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>{'learning_rate': 0.3}</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.897363</td>\n",
       "      <td>0.342930</td>\n",
       "      <td>0.882296</td>\n",
       "      <td>0.387204</td>\n",
       "      <td>2018-12-06 00:50:28</td>\n",
       "      <td>sgd</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>{'learning_rate': 0.3}</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.923340</td>\n",
       "      <td>0.252246</td>\n",
       "      <td>0.910065</td>\n",
       "      <td>0.295503</td>\n",
       "      <td>2018-12-06 00:50:28</td>\n",
       "      <td>sgd</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>{'learning_rate': 0.3}</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.933496</td>\n",
       "      <td>0.213093</td>\n",
       "      <td>0.927072</td>\n",
       "      <td>0.241143</td>\n",
       "      <td>2018-12-06 00:50:28</td>\n",
       "      <td>sgd</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>{'learning_rate': 0.3}</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.939258</td>\n",
       "      <td>0.198226</td>\n",
       "      <td>0.938100</td>\n",
       "      <td>0.205923</td>\n",
       "      <td>2018-12-06 00:50:28</td>\n",
       "      <td>sgd</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>{'learning_rate': 0.3}</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.944922</td>\n",
       "      <td>0.179419</td>\n",
       "      <td>0.943863</td>\n",
       "      <td>0.185448</td>\n",
       "      <td>2018-12-06 00:50:28</td>\n",
       "      <td>sgd</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>{'learning_rate': 0.3}</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.946777</td>\n",
       "      <td>0.169243</td>\n",
       "      <td>0.948028</td>\n",
       "      <td>0.171785</td>\n",
       "      <td>2018-12-06 00:50:28</td>\n",
       "      <td>sgd</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>{'learning_rate': 0.3}</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.947363</td>\n",
       "      <td>0.165610</td>\n",
       "      <td>0.951026</td>\n",
       "      <td>0.161770</td>\n",
       "      <td>2018-12-06 00:50:28</td>\n",
       "      <td>sgd</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>{'learning_rate': 0.3}</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.949609</td>\n",
       "      <td>0.161815</td>\n",
       "      <td>0.953258</td>\n",
       "      <td>0.153310</td>\n",
       "      <td>2018-12-06 00:50:28</td>\n",
       "      <td>sgd</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>{'learning_rate': 0.3}</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.950781</td>\n",
       "      <td>0.157242</td>\n",
       "      <td>0.955840</td>\n",
       "      <td>0.146524</td>\n",
       "      <td>2018-12-06 00:50:28</td>\n",
       "      <td>sgd</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>{'learning_rate': 0.3}</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
       "0  0.876172  0.414475   0.491571    1.864575  2018-12-06 00:50:28       sgd   \n",
       "1  0.897363  0.342930   0.882296    0.387204  2018-12-06 00:50:28       sgd   \n",
       "2  0.923340  0.252246   0.910065    0.295503  2018-12-06 00:50:28       sgd   \n",
       "3  0.933496  0.213093   0.927072    0.241143  2018-12-06 00:50:28       sgd   \n",
       "4  0.939258  0.198226   0.938100    0.205923  2018-12-06 00:50:28       sgd   \n",
       "5  0.944922  0.179419   0.943863    0.185448  2018-12-06 00:50:28       sgd   \n",
       "6  0.946777  0.169243   0.948028    0.171785  2018-12-06 00:50:28       sgd   \n",
       "7  0.947363  0.165610   0.951026    0.161770  2018-12-06 00:50:28       sgd   \n",
       "8  0.949609  0.161815   0.953258    0.153310  2018-12-06 00:50:28       sgd   \n",
       "9  0.950781  0.157242   0.955840    0.146524  2018-12-06 00:50:28       sgd   \n",
       "\n",
       "   is_svrg  batch_size             hyperparams  epoch  update_freq  \n",
       "0    False         128  {'learning_rate': 0.3}      1            1  \n",
       "1    False         128  {'learning_rate': 0.3}      2            1  \n",
       "2    False         128  {'learning_rate': 0.3}      3            1  \n",
       "3    False         128  {'learning_rate': 0.3}      4            1  \n",
       "4    False         128  {'learning_rate': 0.3}      5            1  \n",
       "5    False         128  {'learning_rate': 0.3}      6            1  \n",
       "6    False         128  {'learning_rate': 0.3}      7            1  \n",
       "7    False         128  {'learning_rate': 0.3}      8            1  \n",
       "8    False         128  {'learning_rate': 0.3}      9            1  \n",
       "9    False         128  {'learning_rate': 0.3}     10            1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 128 sgd True 0\n"
     ]
    }
   ],
   "source": [
    "# MLP EXPERIMENT\n",
    "\n",
    "run = True\n",
    "\n",
    "# define hyperparameters\n",
    "epochs = 25\n",
    "batch_size = 128\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "c = 0\n",
    "\n",
    "if run:\n",
    "    for _lr in range(1, 10):\n",
    "        lr = (_lr)/10\n",
    "        for activation in ['relu', 'tanh']:\n",
    "            for optimizer in ['sgd', 'adam', 'rmsprop']:\n",
    "                for is_svrg in [True, False]:\n",
    "                    \n",
    "                    #ctx = mx.gpu()\n",
    "                    print(lr, batch_size, optimizer, is_svrg, c)\n",
    "                    if is_svrg == False:\n",
    "                        ctx = mx.gpu()\n",
    "                        nn = NeuralNetwork(\n",
    "                            dataset='MNIST', optimizer=optimizer, epochs=epochs, context=ctx, batch_size=batch_size, is_svrg=is_svrg,\n",
    "                            hyperparams={'learning_rate': lr})\n",
    "                        nn.run_neural(1, act=activation)\n",
    "                        c = c + 1\n",
    "                        df = pd.concat([df, nn.df_out])\n",
    "                    else:\n",
    "                        ctx = mx.cpu()\n",
    "                        for update_freq in [3, 5, 8, 10]:\n",
    "                            nn = NeuralNetwork(\n",
    "                                dataset='MNIST', optimizer=optimizer, epochs=epochs, context=ctx, batch_size=batch_size, is_svrg=is_svrg,\n",
    "                            hyperparams={'learning_rate': lr})\n",
    "                            nn.run_neural(update_freq, act=activation)\n",
    "                            print(lr, batch_size, optimizer, is_svrg, update_freq, c)\n",
    "                            c = c + 1\n",
    "                                                \n",
    "                            df = pd.concat([df, nn.df_out])\n",
    "                \n",
    "                            df.to_pickle('output_nn.pkl')    \n",
    "                    #print(lr_svrg_sgd.df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 8 sgd True 0\n",
      "2018-12-05 08:22:07\n",
      "0.1 8 sgd True 3 0\n",
      "2018-12-05 08:24:02\n",
      "0.1 8 sgd True 5 1\n",
      "2018-12-05 08:25:55\n",
      "0.1 8 sgd True 8 2\n",
      "2018-12-05 08:27:48\n",
      "0.1 8 sgd True 10 3\n",
      "0.1 8 sgd False 4\n",
      "2018-12-05 08:29:41\n",
      "0.1 8 adam True 5\n",
      "2018-12-05 08:30:09\n",
      "0.1 8 adam True 3 5\n",
      "2018-12-05 08:32:07\n",
      "0.1 8 adam True 5 6\n",
      "2018-12-05 08:34:03\n",
      "0.1 8 adam True 8 7\n",
      "2018-12-05 08:35:58\n",
      "0.1 8 adam True 10 8\n",
      "0.1 8 adam False 9\n",
      "2018-12-05 08:37:54\n",
      "0.1 8 rmsprop True 10\n",
      "2018-12-05 08:38:36\n",
      "0.1 8 rmsprop True 3 10\n",
      "2018-12-05 08:40:32\n",
      "0.1 8 rmsprop True 5 11\n",
      "2018-12-05 08:42:26\n",
      "0.1 8 rmsprop True 8 12\n",
      "2018-12-05 08:44:20\n",
      "0.1 8 rmsprop True 10 13\n",
      "0.1 8 rmsprop False 14\n",
      "2018-12-05 08:46:13\n",
      "0.1 16 sgd True 15\n",
      "2018-12-05 08:46:47\n",
      "0.1 16 sgd True 3 15\n",
      "2018-12-05 08:47:45\n",
      "0.1 16 sgd True 5 16\n",
      "2018-12-05 08:48:42\n",
      "0.1 16 sgd True 8 17\n",
      "2018-12-05 08:49:39\n",
      "0.1 16 sgd True 10 18\n",
      "0.1 16 sgd False 19\n",
      "2018-12-05 08:50:36\n",
      "0.1 16 adam True 20\n",
      "2018-12-05 08:50:50\n",
      "0.1 16 adam True 3 20\n",
      "2018-12-05 08:51:50\n",
      "0.1 16 adam True 5 21\n",
      "2018-12-05 08:52:48\n",
      "0.1 16 adam True 8 22\n",
      "2018-12-05 08:53:47\n",
      "0.1 16 adam True 10 23\n",
      "0.1 16 adam False 24\n",
      "2018-12-05 08:54:45\n",
      "0.1 16 rmsprop True 25\n",
      "2018-12-05 08:55:07\n",
      "0.1 16 rmsprop True 3 25\n",
      "2018-12-05 08:56:05\n",
      "0.1 16 rmsprop True 5 26\n",
      "2018-12-05 08:57:03\n",
      "0.1 16 rmsprop True 8 27\n",
      "2018-12-05 08:58:00\n",
      "0.1 16 rmsprop True 10 28\n",
      "0.1 16 rmsprop False 29\n",
      "2018-12-05 08:58:58\n",
      "0.1 32 sgd True 30\n",
      "2018-12-05 08:59:17\n",
      "0.1 32 sgd True 3 30\n",
      "2018-12-05 08:59:46\n",
      "0.1 32 sgd True 5 31\n",
      "2018-12-05 09:00:15\n",
      "0.1 32 sgd True 8 32\n",
      "2018-12-05 09:00:44\n",
      "0.1 32 sgd True 10 33\n",
      "0.1 32 sgd False 34\n",
      "2018-12-05 09:01:13\n",
      "0.1 32 adam True 35\n",
      "2018-12-05 09:01:21\n",
      "0.1 32 adam True 3 35\n",
      "2018-12-05 09:01:52\n",
      "0.1 32 adam True 5 36\n",
      "2018-12-05 09:02:21\n",
      "0.1 32 adam True 8 37\n",
      "2018-12-05 09:02:51\n",
      "0.1 32 adam True 10 38\n",
      "0.1 32 adam False 39\n",
      "2018-12-05 09:03:21\n",
      "0.1 32 rmsprop True 40\n",
      "2018-12-05 09:03:32\n",
      "0.1 32 rmsprop True 3 40\n",
      "2018-12-05 09:04:02\n",
      "0.1 32 rmsprop True 5 41\n",
      "2018-12-05 09:04:32\n",
      "0.1 32 rmsprop True 8 42\n",
      "2018-12-05 09:05:01\n",
      "0.1 32 rmsprop True 10 43\n",
      "0.1 32 rmsprop False 44\n",
      "2018-12-05 09:05:30\n",
      "0.1 64 sgd True 45\n",
      "2018-12-05 09:05:41\n",
      "0.1 64 sgd True 3 45\n",
      "2018-12-05 09:05:56\n",
      "0.1 64 sgd True 5 46\n",
      "2018-12-05 09:06:12\n",
      "0.1 64 sgd True 8 47\n",
      "2018-12-05 09:06:27\n",
      "0.1 64 sgd True 10 48\n",
      "0.1 64 sgd False 49\n",
      "2018-12-05 09:06:42\n",
      "0.1 64 adam True 50\n",
      "2018-12-05 09:06:47\n",
      "0.1 64 adam True 3 50\n",
      "2018-12-05 09:07:03\n",
      "0.1 64 adam True 5 51\n",
      "2018-12-05 09:07:18\n",
      "0.1 64 adam True 8 52\n",
      "2018-12-05 09:07:34\n",
      "0.1 64 adam True 10 53\n",
      "0.1 64 adam False 54\n",
      "2018-12-05 09:07:49\n",
      "0.1 64 rmsprop True 55\n",
      "2018-12-05 09:07:57\n",
      "0.1 64 rmsprop True 3 55\n",
      "2018-12-05 09:08:13\n",
      "0.1 64 rmsprop True 5 56\n",
      "2018-12-05 09:08:28\n",
      "0.1 64 rmsprop True 8 57\n",
      "2018-12-05 09:08:44\n",
      "0.1 64 rmsprop True 10 58\n",
      "0.1 64 rmsprop False 59\n",
      "2018-12-05 09:08:59\n",
      "0.2 8 sgd True 60\n",
      "2018-12-05 09:09:06\n",
      "0.2 8 sgd True 3 60\n",
      "2018-12-05 09:11:01\n",
      "0.2 8 sgd True 5 61\n",
      "2018-12-05 09:12:54\n",
      "0.2 8 sgd True 8 62\n",
      "2018-12-05 09:14:47\n",
      "0.2 8 sgd True 10 63\n",
      "0.2 8 sgd False 64\n",
      "2018-12-05 09:16:40\n",
      "0.2 8 adam True 65\n",
      "2018-12-05 09:17:08\n",
      "0.2 8 adam True 3 65\n",
      "2018-12-05 09:19:06\n",
      "0.2 8 adam True 5 66\n",
      "2018-12-05 09:21:02\n",
      "0.2 8 adam True 8 67\n",
      "2018-12-05 09:22:58\n",
      "0.2 8 adam True 10 68\n",
      "0.2 8 adam False 69\n",
      "2018-12-05 09:24:53\n",
      "0.2 8 rmsprop True 70\n",
      "2018-12-05 09:25:38\n",
      "0.2 8 rmsprop True 3 70\n",
      "2018-12-05 09:27:34\n",
      "0.2 8 rmsprop True 5 71\n",
      "2018-12-05 09:29:29\n",
      "0.2 8 rmsprop True 8 72\n",
      "2018-12-05 09:31:23\n",
      "0.2 8 rmsprop True 10 73\n",
      "0.2 8 rmsprop False 74\n",
      "2018-12-05 09:33:16\n",
      "0.2 16 sgd True 75\n",
      "2018-12-05 09:33:48\n",
      "0.2 16 sgd True 3 75\n",
      "2018-12-05 09:34:46\n",
      "0.2 16 sgd True 5 76\n",
      "2018-12-05 09:35:43\n",
      "0.2 16 sgd True 8 77\n",
      "2018-12-05 09:36:40\n",
      "0.2 16 sgd True 10 78\n",
      "0.2 16 sgd False 79\n",
      "2018-12-05 09:37:37\n",
      "0.2 16 adam True 80\n",
      "2018-12-05 09:37:51\n",
      "0.2 16 adam True 3 80\n",
      "2018-12-05 09:38:50\n",
      "0.2 16 adam True 5 81\n",
      "2018-12-05 09:39:49\n",
      "0.2 16 adam True 8 82\n",
      "2018-12-05 09:40:47\n",
      "0.2 16 adam True 10 83\n",
      "0.2 16 adam False 84\n",
      "2018-12-05 09:41:45\n",
      "0.2 16 rmsprop True 85\n",
      "2018-12-05 09:42:08\n",
      "0.2 16 rmsprop True 3 85\n",
      "2018-12-05 09:43:07\n",
      "0.2 16 rmsprop True 5 86\n",
      "2018-12-05 09:44:04\n",
      "0.2 16 rmsprop True 8 87\n",
      "2018-12-05 09:45:01\n",
      "0.2 16 rmsprop True 10 88\n",
      "0.2 16 rmsprop False 89\n",
      "2018-12-05 09:45:58\n",
      "0.2 32 sgd True 90\n",
      "2018-12-05 09:46:16\n",
      "0.2 32 sgd True 3 90\n",
      "2018-12-05 09:46:46\n",
      "0.2 32 sgd True 5 91\n",
      "2018-12-05 09:47:15\n",
      "0.2 32 sgd True 8 92\n",
      "2018-12-05 09:47:44\n",
      "0.2 32 sgd True 10 93\n",
      "0.2 32 sgd False 94\n",
      "2018-12-05 09:48:13\n",
      "0.2 32 adam True 95\n",
      "2018-12-05 09:48:21\n",
      "0.2 32 adam True 3 95\n",
      "2018-12-05 09:48:51\n",
      "0.2 32 adam True 5 96\n",
      "2018-12-05 09:49:21\n",
      "0.2 32 adam True 8 97\n",
      "2018-12-05 09:49:51\n",
      "0.2 32 adam True 10 98\n",
      "0.2 32 adam False 99\n",
      "2018-12-05 09:50:21\n",
      "0.2 32 rmsprop True 100\n",
      "2018-12-05 09:50:33\n",
      "0.2 32 rmsprop True 3 100\n",
      "2018-12-05 09:51:03\n",
      "0.2 32 rmsprop True 5 101\n",
      "2018-12-05 09:51:33\n",
      "0.2 32 rmsprop True 8 102\n",
      "2018-12-05 09:52:02\n",
      "0.2 32 rmsprop True 10 103\n",
      "0.2 32 rmsprop False 104\n",
      "2018-12-05 09:52:31\n",
      "0.2 64 sgd True 105\n",
      "2018-12-05 09:52:41\n",
      "0.2 64 sgd True 3 105\n",
      "2018-12-05 09:52:57\n",
      "0.2 64 sgd True 5 106\n",
      "2018-12-05 09:53:12\n",
      "0.2 64 sgd True 8 107\n",
      "2018-12-05 09:53:27\n",
      "0.2 64 sgd True 10 108\n",
      "0.2 64 sgd False 109\n",
      "2018-12-05 09:53:42\n",
      "0.2 64 adam True 110\n",
      "2018-12-05 09:53:47\n",
      "0.2 64 adam True 3 110\n",
      "2018-12-05 09:54:03\n",
      "0.2 64 adam True 5 111\n",
      "2018-12-05 09:54:19\n",
      "0.2 64 adam True 8 112\n",
      "2018-12-05 09:54:34\n",
      "0.2 64 adam True 10 113\n",
      "0.2 64 adam False 114\n",
      "2018-12-05 09:54:50\n",
      "0.2 64 rmsprop True 115\n",
      "2018-12-05 09:54:57\n",
      "0.2 64 rmsprop True 3 115\n",
      "2018-12-05 09:55:13\n",
      "0.2 64 rmsprop True 5 116\n",
      "2018-12-05 09:55:28\n",
      "0.2 64 rmsprop True 8 117\n",
      "2018-12-05 09:55:44\n",
      "0.2 64 rmsprop True 10 118\n",
      "0.2 64 rmsprop False 119\n",
      "2018-12-05 09:55:59\n",
      "0.3 8 sgd True 120\n",
      "2018-12-05 09:56:06\n",
      "0.3 8 sgd True 3 120\n",
      "2018-12-05 09:58:02\n",
      "0.3 8 sgd True 5 121\n",
      "2018-12-05 09:59:56\n",
      "0.3 8 sgd True 8 122\n",
      "2018-12-05 10:01:50\n",
      "0.3 8 sgd True 10 123\n",
      "0.3 8 sgd False 124\n",
      "2018-12-05 10:03:42\n",
      "0.3 8 adam True 125\n",
      "2018-12-05 10:04:11\n",
      "0.3 8 adam True 3 125\n",
      "2018-12-05 10:06:09\n",
      "0.3 8 adam True 5 126\n",
      "2018-12-05 10:08:05\n",
      "0.3 8 adam True 8 127\n",
      "2018-12-05 10:10:01\n",
      "0.3 8 adam True 10 128\n",
      "0.3 8 adam False 129\n",
      "2018-12-05 10:11:56\n",
      "0.3 8 rmsprop True 130\n",
      "2018-12-05 10:12:41\n",
      "0.3 8 rmsprop True 3 130\n",
      "2018-12-05 10:14:37\n",
      "0.3 8 rmsprop True 5 131\n",
      "2018-12-05 10:16:32\n",
      "0.3 8 rmsprop True 8 132\n",
      "2018-12-05 10:18:26\n",
      "0.3 8 rmsprop True 10 133\n",
      "0.3 8 rmsprop False 134\n",
      "2018-12-05 10:20:19\n",
      "0.3 16 sgd True 135\n",
      "2018-12-05 10:20:50\n",
      "0.3 16 sgd True 3 135\n",
      "2018-12-05 10:21:48\n",
      "0.3 16 sgd True 5 136\n",
      "2018-12-05 10:22:45\n",
      "0.3 16 sgd True 8 137\n",
      "2018-12-05 10:23:43\n",
      "0.3 16 sgd True 10 138\n",
      "0.3 16 sgd False 139\n",
      "2018-12-05 10:24:39\n",
      "0.3 16 adam True 140\n",
      "2018-12-05 10:24:54\n",
      "0.3 16 adam True 3 140\n",
      "2018-12-05 10:25:54\n",
      "0.3 16 adam True 5 141\n",
      "2018-12-05 10:26:52\n",
      "0.3 16 adam True 8 142\n",
      "2018-12-05 10:27:51\n",
      "0.3 16 adam True 10 143\n",
      "0.3 16 adam False 144\n",
      "2018-12-05 10:28:49\n",
      "0.3 16 rmsprop True 145\n",
      "2018-12-05 10:29:12\n",
      "0.3 16 rmsprop True 3 145\n",
      "2018-12-05 10:30:10\n",
      "0.3 16 rmsprop True 5 146\n",
      "2018-12-05 10:31:08\n",
      "0.3 16 rmsprop True 8 147\n",
      "2018-12-05 10:32:05\n",
      "0.3 16 rmsprop True 10 148\n",
      "0.3 16 rmsprop False 149\n",
      "2018-12-05 10:33:02\n",
      "0.3 32 sgd True 150\n",
      "2018-12-05 10:33:19\n",
      "0.3 32 sgd True 3 150\n",
      "2018-12-05 10:33:49\n",
      "0.3 32 sgd True 5 151\n",
      "2018-12-05 10:34:18\n",
      "0.3 32 sgd True 8 152\n",
      "2018-12-05 10:34:47\n",
      "0.3 32 sgd True 10 153\n",
      "0.3 32 sgd False 154\n",
      "2018-12-05 10:35:16\n",
      "0.3 32 adam True 155\n",
      "2018-12-05 10:35:24\n",
      "0.3 32 adam True 3 155\n",
      "2018-12-05 10:35:54\n",
      "0.3 32 adam True 5 156\n",
      "2018-12-05 10:36:23\n",
      "0.3 32 adam True 8 157\n",
      "2018-12-05 10:36:53\n",
      "0.3 32 adam True 10 158\n",
      "0.3 32 adam False 159\n",
      "2018-12-05 10:37:23\n",
      "0.3 32 rmsprop True 160\n",
      "2018-12-05 10:37:35\n",
      "0.3 32 rmsprop True 3 160\n",
      "2018-12-05 10:38:05\n",
      "0.3 32 rmsprop True 5 161\n",
      "2018-12-05 10:38:35\n",
      "0.3 32 rmsprop True 8 162\n",
      "2018-12-05 10:39:04\n",
      "0.3 32 rmsprop True 10 163\n",
      "0.3 32 rmsprop False 164\n",
      "2018-12-05 10:39:33\n",
      "0.3 64 sgd True 165\n",
      "2018-12-05 10:39:44\n",
      "0.3 64 sgd True 3 165\n",
      "2018-12-05 10:39:59\n",
      "0.3 64 sgd True 5 166\n",
      "2018-12-05 10:40:14\n",
      "0.3 64 sgd True 8 167\n",
      "2018-12-05 10:40:30\n",
      "0.3 64 sgd True 10 168\n",
      "0.3 64 sgd False 169\n",
      "2018-12-05 10:40:45\n",
      "0.3 64 adam True 170\n",
      "2018-12-05 10:40:50\n",
      "0.3 64 adam True 3 170\n",
      "2018-12-05 10:41:06\n",
      "0.3 64 adam True 5 171\n",
      "2018-12-05 10:41:21\n",
      "0.3 64 adam True 8 172\n",
      "2018-12-05 10:41:37\n",
      "0.3 64 adam True 10 173\n",
      "0.3 64 adam False 174\n",
      "2018-12-05 10:41:52\n",
      "0.3 64 rmsprop True 175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-05 10:42:00\n",
      "0.3 64 rmsprop True 3 175\n",
      "2018-12-05 10:42:16\n",
      "0.3 64 rmsprop True 5 176\n",
      "2018-12-05 10:42:31\n",
      "0.3 64 rmsprop True 8 177\n",
      "2018-12-05 10:42:47\n",
      "0.3 64 rmsprop True 10 178\n",
      "0.3 64 rmsprop False 179\n",
      "2018-12-05 10:43:02\n",
      "0.4 8 sgd True 180\n",
      "2018-12-05 10:43:09\n",
      "0.4 8 sgd True 3 180\n",
      "2018-12-05 10:45:04\n",
      "0.4 8 sgd True 5 181\n",
      "2018-12-05 10:46:57\n",
      "0.4 8 sgd True 8 182\n",
      "2018-12-05 10:48:50\n",
      "0.4 8 sgd True 10 183\n",
      "0.4 8 sgd False 184\n",
      "2018-12-05 10:50:42\n",
      "0.4 8 adam True 185\n",
      "2018-12-05 10:51:11\n",
      "0.4 8 adam True 3 185\n",
      "2018-12-05 10:53:09\n",
      "0.4 8 adam True 5 186\n",
      "2018-12-05 10:55:05\n",
      "0.4 8 adam True 8 187\n",
      "2018-12-05 10:57:02\n",
      "0.4 8 adam True 10 188\n",
      "0.4 8 adam False 189\n",
      "2018-12-05 10:58:57\n",
      "0.4 8 rmsprop True 190\n",
      "2018-12-05 10:59:41\n",
      "0.4 8 rmsprop True 3 190\n",
      "2018-12-05 11:01:37\n",
      "0.4 8 rmsprop True 5 191\n",
      "2018-12-05 11:03:32\n",
      "0.4 8 rmsprop True 8 192\n",
      "2018-12-05 11:05:26\n",
      "0.4 8 rmsprop True 10 193\n",
      "0.4 8 rmsprop False 194\n",
      "2018-12-05 11:07:19\n",
      "0.4 16 sgd True 195\n",
      "2018-12-05 11:07:49\n",
      "0.4 16 sgd True 3 195\n",
      "2018-12-05 11:08:48\n",
      "0.4 16 sgd True 5 196\n",
      "2018-12-05 11:09:45\n",
      "0.4 16 sgd True 8 197\n",
      "2018-12-05 11:10:42\n",
      "0.4 16 sgd True 10 198\n",
      "0.4 16 sgd False 199\n",
      "2018-12-05 11:11:39\n",
      "0.4 16 adam True 200\n",
      "2018-12-05 11:11:53\n",
      "0.4 16 adam True 3 200\n",
      "2018-12-05 11:12:53\n",
      "0.4 16 adam True 5 201\n",
      "2018-12-05 11:13:51\n",
      "0.4 16 adam True 8 202\n",
      "2018-12-05 11:14:50\n",
      "0.4 16 adam True 10 203\n",
      "0.4 16 adam False 204\n",
      "2018-12-05 11:15:48\n",
      "0.4 16 rmsprop True 205\n",
      "2018-12-05 11:16:10\n",
      "0.4 16 rmsprop True 3 205\n",
      "2018-12-05 11:17:09\n",
      "0.4 16 rmsprop True 5 206\n",
      "2018-12-05 11:18:06\n",
      "0.4 16 rmsprop True 8 207\n",
      "2018-12-05 11:19:04\n",
      "0.4 16 rmsprop True 10 208\n",
      "0.4 16 rmsprop False 209\n",
      "2018-12-05 11:20:01\n",
      "0.4 32 sgd True 210\n",
      "2018-12-05 11:20:18\n",
      "0.4 32 sgd True 3 210\n",
      "2018-12-05 11:20:47\n",
      "0.4 32 sgd True 5 211\n",
      "2018-12-05 11:21:16\n",
      "0.4 32 sgd True 8 212\n",
      "2018-12-05 11:21:46\n",
      "0.4 32 sgd True 10 213\n",
      "0.4 32 sgd False 214\n",
      "2018-12-05 11:22:14\n",
      "0.4 32 adam True 215\n",
      "2018-12-05 11:22:22\n",
      "0.4 32 adam True 3 215\n",
      "2018-12-05 11:22:53\n",
      "0.4 32 adam True 5 216\n",
      "2018-12-05 11:23:22\n",
      "0.4 32 adam True 8 217\n",
      "2018-12-05 11:23:52\n",
      "0.4 32 adam True 10 218\n",
      "0.4 32 adam False 219\n",
      "2018-12-05 11:24:22\n",
      "0.4 32 rmsprop True 220\n",
      "2018-12-05 11:24:34\n",
      "0.4 32 rmsprop True 3 220\n",
      "2018-12-05 11:25:04\n",
      "0.4 32 rmsprop True 5 221\n",
      "2018-12-05 11:25:33\n",
      "0.4 32 rmsprop True 8 222\n",
      "2018-12-05 11:26:02\n",
      "0.4 32 rmsprop True 10 223\n",
      "0.4 32 rmsprop False 224\n",
      "2018-12-05 11:26:31\n",
      "0.4 64 sgd True 225\n",
      "2018-12-05 11:26:41\n",
      "0.4 64 sgd True 3 225\n",
      "2018-12-05 11:26:57\n",
      "0.4 64 sgd True 5 226\n",
      "2018-12-05 11:27:12\n",
      "0.4 64 sgd True 8 227\n",
      "2018-12-05 11:27:27\n",
      "0.4 64 sgd True 10 228\n",
      "0.4 64 sgd False 229\n",
      "2018-12-05 11:27:42\n",
      "0.4 64 adam True 230\n",
      "2018-12-05 11:27:47\n",
      "0.4 64 adam True 3 230\n",
      "2018-12-05 11:28:03\n",
      "0.4 64 adam True 5 231\n",
      "2018-12-05 11:28:19\n",
      "0.4 64 adam True 8 232\n",
      "2018-12-05 11:28:34\n",
      "0.4 64 adam True 10 233\n",
      "0.4 64 adam False 234\n",
      "2018-12-05 11:28:50\n",
      "0.4 64 rmsprop True 235\n",
      "2018-12-05 11:28:57\n",
      "0.4 64 rmsprop True 3 235\n",
      "2018-12-05 11:29:13\n",
      "0.4 64 rmsprop True 5 236\n",
      "2018-12-05 11:29:28\n",
      "0.4 64 rmsprop True 8 237\n",
      "2018-12-05 11:29:43\n",
      "0.4 64 rmsprop True 10 238\n",
      "0.4 64 rmsprop False 239\n",
      "2018-12-05 11:29:59\n",
      "0.5 8 sgd True 240\n",
      "2018-12-05 11:30:05\n",
      "0.5 8 sgd True 3 240\n",
      "2018-12-05 11:32:01\n",
      "0.5 8 sgd True 5 241\n",
      "2018-12-05 11:33:54\n",
      "0.5 8 sgd True 8 242\n",
      "2018-12-05 11:35:48\n",
      "0.5 8 sgd True 10 243\n",
      "0.5 8 sgd False 244\n",
      "2018-12-05 11:37:40\n",
      "0.5 8 adam True 245\n",
      "2018-12-05 11:38:09\n",
      "0.5 8 adam True 3 245\n",
      "2018-12-05 11:40:07\n",
      "0.5 8 adam True 5 246\n",
      "2018-12-05 11:42:03\n",
      "0.5 8 adam True 8 247\n",
      "2018-12-05 11:44:00\n",
      "0.5 8 adam True 10 248\n",
      "0.5 8 adam False 249\n",
      "2018-12-05 11:45:55\n",
      "0.5 8 rmsprop True 250\n",
      "2018-12-05 11:46:38\n",
      "0.5 8 rmsprop True 3 250\n",
      "2018-12-05 11:48:35\n",
      "0.5 8 rmsprop True 5 251\n",
      "2018-12-05 11:50:30\n",
      "0.5 8 rmsprop True 8 252\n",
      "2018-12-05 11:52:24\n",
      "0.5 8 rmsprop True 10 253\n",
      "0.5 8 rmsprop False 254\n",
      "2018-12-05 11:54:18\n",
      "0.5 16 sgd True 255\n",
      "2018-12-05 11:54:48\n",
      "0.5 16 sgd True 3 255\n",
      "2018-12-05 11:55:47\n",
      "0.5 16 sgd True 5 256\n",
      "2018-12-05 11:56:44\n",
      "0.5 16 sgd True 8 257\n",
      "2018-12-05 11:57:41\n",
      "0.5 16 sgd True 10 258\n",
      "0.5 16 sgd False 259\n",
      "2018-12-05 11:58:37\n",
      "0.5 16 adam True 260\n",
      "2018-12-05 11:58:52\n",
      "0.5 16 adam True 3 260\n",
      "2018-12-05 11:59:51\n",
      "0.5 16 adam True 5 261\n",
      "2018-12-05 12:00:50\n",
      "0.5 16 adam True 8 262\n",
      "2018-12-05 12:01:49\n",
      "0.5 16 adam True 10 263\n",
      "0.5 16 adam False 264\n",
      "2018-12-05 12:02:47\n",
      "0.5 16 rmsprop True 265\n",
      "2018-12-05 12:03:09\n",
      "0.5 16 rmsprop True 3 265\n",
      "2018-12-05 12:04:07\n",
      "0.5 16 rmsprop True 5 266\n",
      "2018-12-05 12:05:05\n",
      "0.5 16 rmsprop True 8 267\n",
      "2018-12-05 12:06:02\n",
      "0.5 16 rmsprop True 10 268\n",
      "0.5 16 rmsprop False 269\n",
      "2018-12-05 12:06:59\n",
      "0.5 32 sgd True 270\n",
      "2018-12-05 12:07:16\n",
      "0.5 32 sgd True 3 270\n",
      "2018-12-05 12:07:45\n",
      "0.5 32 sgd True 5 271\n",
      "2018-12-05 12:08:14\n",
      "0.5 32 sgd True 8 272\n",
      "2018-12-05 12:08:43\n",
      "0.5 32 sgd True 10 273\n",
      "0.5 32 sgd False 274\n",
      "2018-12-05 12:09:12\n",
      "0.5 32 adam True 275\n",
      "2018-12-05 12:09:20\n",
      "0.5 32 adam True 3 275\n",
      "2018-12-05 12:09:51\n",
      "0.5 32 adam True 5 276\n",
      "2018-12-05 12:10:20\n",
      "0.5 32 adam True 8 277\n",
      "2018-12-05 12:10:50\n",
      "0.5 32 adam True 10 278\n",
      "0.5 32 adam False 279\n",
      "2018-12-05 12:11:20\n",
      "0.5 32 rmsprop True 280\n",
      "2018-12-05 12:11:32\n",
      "0.5 32 rmsprop True 3 280\n",
      "2018-12-05 12:12:02\n",
      "0.5 32 rmsprop True 5 281\n",
      "2018-12-05 12:12:31\n",
      "0.5 32 rmsprop True 8 282\n",
      "2018-12-05 12:13:00\n",
      "0.5 32 rmsprop True 10 283\n",
      "0.5 32 rmsprop False 284\n",
      "2018-12-05 12:13:29\n",
      "0.5 64 sgd True 285\n",
      "2018-12-05 12:13:39\n",
      "0.5 64 sgd True 3 285\n",
      "2018-12-05 12:13:55\n",
      "0.5 64 sgd True 5 286\n",
      "2018-12-05 12:14:10\n",
      "0.5 64 sgd True 8 287\n",
      "2018-12-05 12:14:25\n",
      "0.5 64 sgd True 10 288\n",
      "0.5 64 sgd False 289\n",
      "2018-12-05 12:14:40\n",
      "0.5 64 adam True 290\n",
      "2018-12-05 12:14:45\n",
      "0.5 64 adam True 3 290\n",
      "2018-12-05 12:15:01\n",
      "0.5 64 adam True 5 291\n",
      "2018-12-05 12:15:17\n",
      "0.5 64 adam True 8 292\n",
      "2018-12-05 12:15:32\n",
      "0.5 64 adam True 10 293\n",
      "0.5 64 adam False 294\n",
      "2018-12-05 12:15:47\n",
      "0.5 64 rmsprop True 295\n",
      "2018-12-05 12:15:55\n",
      "0.5 64 rmsprop True 3 295\n",
      "2018-12-05 12:16:10\n",
      "0.5 64 rmsprop True 5 296\n",
      "2018-12-05 12:16:26\n",
      "0.5 64 rmsprop True 8 297\n",
      "2018-12-05 12:16:41\n",
      "0.5 64 rmsprop True 10 298\n",
      "0.5 64 rmsprop False 299\n",
      "2018-12-05 12:16:56\n",
      "0.6 8 sgd True 300\n",
      "2018-12-05 12:17:02\n",
      "0.6 8 sgd True 3 300\n",
      "2018-12-05 12:18:58\n",
      "0.6 8 sgd True 5 301\n",
      "2018-12-05 12:20:51\n",
      "0.6 8 sgd True 8 302\n",
      "2018-12-05 12:22:44\n",
      "0.6 8 sgd True 10 303\n",
      "0.6 8 sgd False 304\n",
      "2018-12-05 12:24:36\n",
      "0.6 8 adam True 305\n",
      "2018-12-05 12:25:05\n",
      "0.6 8 adam True 3 305\n",
      "2018-12-05 12:27:03\n",
      "0.6 8 adam True 5 306\n",
      "2018-12-05 12:28:59\n",
      "0.6 8 adam True 8 307\n",
      "2018-12-05 12:30:55\n",
      "0.6 8 adam True 10 308\n",
      "0.6 8 adam False 309\n",
      "2018-12-05 12:32:50\n",
      "0.6 8 rmsprop True 310\n",
      "2018-12-05 12:33:33\n",
      "0.6 8 rmsprop True 3 310\n",
      "2018-12-05 12:35:29\n",
      "0.6 8 rmsprop True 5 311\n",
      "2018-12-05 12:37:23\n",
      "0.6 8 rmsprop True 8 312\n",
      "2018-12-05 12:39:17\n",
      "0.6 8 rmsprop True 10 313\n",
      "0.6 8 rmsprop False 314\n",
      "2018-12-05 12:41:11\n",
      "0.6 16 sgd True 315\n",
      "2018-12-05 12:41:40\n",
      "0.6 16 sgd True 3 315\n",
      "2018-12-05 12:42:38\n",
      "0.6 16 sgd True 5 316\n",
      "2018-12-05 12:43:35\n",
      "0.6 16 sgd True 8 317\n",
      "2018-12-05 12:44:32\n",
      "0.6 16 sgd True 10 318\n",
      "0.6 16 sgd False 319\n",
      "2018-12-05 12:45:29\n",
      "0.6 16 adam True 320\n",
      "2018-12-05 12:45:44\n",
      "0.6 16 adam True 3 320\n",
      "2018-12-05 12:46:43\n",
      "0.6 16 adam True 5 321\n",
      "2018-12-05 12:47:42\n",
      "0.6 16 adam True 8 322\n",
      "2018-12-05 12:48:41\n",
      "0.6 16 adam True 10 323\n",
      "0.6 16 adam False 324\n",
      "2018-12-05 12:49:39\n",
      "0.6 16 rmsprop True 325\n",
      "2018-12-05 12:50:00\n",
      "0.6 16 rmsprop True 3 325\n",
      "2018-12-05 12:50:59\n",
      "0.6 16 rmsprop True 5 326\n",
      "2018-12-05 12:51:56\n",
      "0.6 16 rmsprop True 8 327\n",
      "2018-12-05 12:52:54\n",
      "0.6 16 rmsprop True 10 328\n",
      "0.6 16 rmsprop False 329\n",
      "2018-12-05 12:53:51\n",
      "0.6 32 sgd True 330\n",
      "2018-12-05 12:54:07\n",
      "0.6 32 sgd True 3 330\n",
      "2018-12-05 12:54:36\n",
      "0.6 32 sgd True 5 331\n",
      "2018-12-05 12:55:05\n",
      "0.6 32 sgd True 8 332\n",
      "2018-12-05 12:55:35\n",
      "0.6 32 sgd True 10 333\n",
      "0.6 32 sgd False 334\n",
      "2018-12-05 12:56:03\n",
      "0.6 32 adam True 335\n",
      "2018-12-05 12:56:11\n",
      "0.6 32 adam True 3 335\n",
      "2018-12-05 12:56:42\n",
      "0.6 32 adam True 5 336\n",
      "2018-12-05 12:57:12\n",
      "0.6 32 adam True 8 337\n",
      "2018-12-05 12:57:42\n",
      "0.6 32 adam True 10 338\n",
      "0.6 32 adam False 339\n",
      "2018-12-05 12:58:11\n",
      "0.6 32 rmsprop True 340\n",
      "2018-12-05 12:58:23\n",
      "0.6 32 rmsprop True 3 340\n",
      "2018-12-05 12:58:53\n",
      "0.6 32 rmsprop True 5 341\n",
      "2018-12-05 12:59:22\n",
      "0.6 32 rmsprop True 8 342\n",
      "2018-12-05 12:59:51\n",
      "0.6 32 rmsprop True 10 343\n",
      "0.6 32 rmsprop False 344\n",
      "2018-12-05 13:00:20\n",
      "0.6 64 sgd True 345\n",
      "2018-12-05 13:00:30\n",
      "0.6 64 sgd True 3 345\n",
      "2018-12-05 13:00:46\n",
      "0.6 64 sgd True 5 346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-05 13:01:01\n",
      "0.6 64 sgd True 8 347\n",
      "2018-12-05 13:01:16\n",
      "0.6 64 sgd True 10 348\n",
      "0.6 64 sgd False 349\n",
      "2018-12-05 13:01:31\n",
      "0.6 64 adam True 350\n",
      "2018-12-05 13:01:36\n",
      "0.6 64 adam True 3 350\n",
      "2018-12-05 13:01:52\n",
      "0.6 64 adam True 5 351\n",
      "2018-12-05 13:02:08\n",
      "0.6 64 adam True 8 352\n",
      "2018-12-05 13:02:24\n",
      "0.6 64 adam True 10 353\n",
      "0.6 64 adam False 354\n",
      "2018-12-05 13:02:39\n",
      "0.6 64 rmsprop True 355\n",
      "2018-12-05 13:02:46\n",
      "0.6 64 rmsprop True 3 355\n",
      "2018-12-05 13:03:02\n",
      "0.6 64 rmsprop True 5 356\n",
      "2018-12-05 13:03:17\n",
      "0.6 64 rmsprop True 8 357\n",
      "2018-12-05 13:03:33\n",
      "0.6 64 rmsprop True 10 358\n",
      "0.6 64 rmsprop False 359\n",
      "2018-12-05 13:03:48\n",
      "0.7 8 sgd True 360\n",
      "2018-12-05 13:03:54\n",
      "0.7 8 sgd True 3 360\n",
      "2018-12-05 13:05:50\n",
      "0.7 8 sgd True 5 361\n",
      "2018-12-05 13:07:43\n",
      "0.7 8 sgd True 8 362\n",
      "2018-12-05 13:09:37\n",
      "0.7 8 sgd True 10 363\n",
      "0.7 8 sgd False 364\n",
      "2018-12-05 13:11:29\n",
      "0.7 8 adam True 365\n",
      "2018-12-05 13:11:58\n",
      "0.7 8 adam True 3 365\n",
      "2018-12-05 13:13:56\n",
      "0.7 8 adam True 5 366\n",
      "2018-12-05 13:15:54\n",
      "0.7 8 adam True 8 367\n",
      "2018-12-05 13:17:50\n",
      "0.7 8 adam True 10 368\n",
      "0.7 8 adam False 369\n",
      "2018-12-05 13:19:45\n",
      "0.7 8 rmsprop True 370\n",
      "2018-12-05 13:20:27\n",
      "0.7 8 rmsprop True 3 370\n",
      "2018-12-05 13:22:24\n",
      "0.7 8 rmsprop True 5 371\n",
      "2018-12-05 13:24:19\n",
      "0.7 8 rmsprop True 8 372\n",
      "2018-12-05 13:26:13\n",
      "0.7 8 rmsprop True 10 373\n",
      "0.7 8 rmsprop False 374\n",
      "2018-12-05 13:28:07\n",
      "0.7 16 sgd True 375\n",
      "2018-12-05 13:28:37\n",
      "0.7 16 sgd True 3 375\n",
      "2018-12-05 13:29:35\n",
      "0.7 16 sgd True 5 376\n",
      "2018-12-05 13:30:32\n",
      "0.7 16 sgd True 8 377\n",
      "2018-12-05 13:31:30\n",
      "0.7 16 sgd True 10 378\n",
      "0.7 16 sgd False 379\n",
      "2018-12-05 13:32:26\n",
      "0.7 16 adam True 380\n",
      "2018-12-05 13:32:41\n",
      "0.7 16 adam True 3 380\n",
      "2018-12-05 13:33:41\n",
      "0.7 16 adam True 5 381\n",
      "2018-12-05 13:34:40\n",
      "0.7 16 adam True 8 382\n",
      "2018-12-05 13:35:38\n",
      "0.7 16 adam True 10 383\n",
      "0.7 16 adam False 384\n",
      "2018-12-05 13:36:37\n",
      "0.7 16 rmsprop True 385\n",
      "2018-12-05 13:36:58\n",
      "0.7 16 rmsprop True 3 385\n",
      "2018-12-05 13:37:56\n",
      "0.7 16 rmsprop True 5 386\n",
      "2018-12-05 13:38:54\n",
      "0.7 16 rmsprop True 8 387\n",
      "2018-12-05 13:39:52\n",
      "0.7 16 rmsprop True 10 388\n",
      "0.7 16 rmsprop False 389\n",
      "2018-12-05 13:40:49\n",
      "0.7 32 sgd True 390\n",
      "2018-12-05 13:41:05\n",
      "0.7 32 sgd True 3 390\n",
      "2018-12-05 13:41:34\n",
      "0.7 32 sgd True 5 391\n",
      "2018-12-05 13:42:04\n",
      "0.7 32 sgd True 8 392\n",
      "2018-12-05 13:42:33\n",
      "0.7 32 sgd True 10 393\n",
      "0.7 32 sgd False 394\n",
      "2018-12-05 13:43:02\n",
      "0.7 32 adam True 395\n",
      "2018-12-05 13:43:10\n",
      "0.7 32 adam True 3 395\n",
      "2018-12-05 13:43:40\n",
      "0.7 32 adam True 5 396\n",
      "2018-12-05 13:44:10\n",
      "0.7 32 adam True 8 397\n",
      "2018-12-05 13:44:40\n",
      "0.7 32 adam True 10 398\n",
      "0.7 32 adam False 399\n",
      "2018-12-05 13:45:10\n",
      "0.7 32 rmsprop True 400\n",
      "2018-12-05 13:45:22\n",
      "0.7 32 rmsprop True 3 400\n",
      "2018-12-05 13:45:51\n",
      "0.7 32 rmsprop True 5 401\n",
      "2018-12-05 13:46:21\n",
      "0.7 32 rmsprop True 8 402\n",
      "2018-12-05 13:46:50\n",
      "0.7 32 rmsprop True 10 403\n",
      "0.7 32 rmsprop False 404\n",
      "2018-12-05 13:47:19\n",
      "0.7 64 sgd True 405\n",
      "2018-12-05 13:47:29\n",
      "0.7 64 sgd True 3 405\n",
      "2018-12-05 13:47:44\n",
      "0.7 64 sgd True 5 406\n",
      "2018-12-05 13:48:00\n",
      "0.7 64 sgd True 8 407\n",
      "2018-12-05 13:48:15\n",
      "0.7 64 sgd True 10 408\n",
      "0.7 64 sgd False 409\n",
      "2018-12-05 13:48:30\n",
      "0.7 64 adam True 410\n",
      "2018-12-05 13:48:35\n",
      "0.7 64 adam True 3 410\n",
      "2018-12-05 13:48:51\n",
      "0.7 64 adam True 5 411\n",
      "2018-12-05 13:49:06\n",
      "0.7 64 adam True 8 412\n",
      "2018-12-05 13:49:22\n",
      "0.7 64 adam True 10 413\n",
      "0.7 64 adam False 414\n",
      "2018-12-05 13:49:37\n",
      "0.7 64 rmsprop True 415\n",
      "2018-12-05 13:49:44\n",
      "0.7 64 rmsprop True 3 415\n",
      "2018-12-05 13:50:00\n",
      "0.7 64 rmsprop True 5 416\n",
      "2018-12-05 13:50:15\n",
      "0.7 64 rmsprop True 8 417\n",
      "2018-12-05 13:50:31\n",
      "0.7 64 rmsprop True 10 418\n",
      "0.7 64 rmsprop False 419\n",
      "2018-12-05 13:50:46\n",
      "0.8 8 sgd True 420\n",
      "2018-12-05 13:50:52\n",
      "0.8 8 sgd True 3 420\n",
      "2018-12-05 13:52:47\n",
      "0.8 8 sgd True 5 421\n",
      "2018-12-05 13:54:41\n",
      "0.8 8 sgd True 8 422\n",
      "2018-12-05 13:56:35\n",
      "0.8 8 sgd True 10 423\n",
      "0.8 8 sgd False 424\n",
      "2018-12-05 13:58:27\n",
      "0.8 8 adam True 425\n",
      "2018-12-05 13:58:56\n",
      "0.8 8 adam True 3 425\n",
      "2018-12-05 14:00:54\n",
      "0.8 8 adam True 5 426\n",
      "2018-12-05 14:02:50\n",
      "0.8 8 adam True 8 427\n",
      "2018-12-05 14:04:46\n",
      "0.8 8 adam True 10 428\n",
      "0.8 8 adam False 429\n",
      "2018-12-05 14:06:41\n",
      "0.8 8 rmsprop True 430\n",
      "2018-12-05 14:07:24\n",
      "0.8 8 rmsprop True 3 430\n",
      "2018-12-05 14:09:20\n",
      "0.8 8 rmsprop True 5 431\n",
      "2018-12-05 14:11:15\n",
      "0.8 8 rmsprop True 8 432\n",
      "2018-12-05 14:13:10\n",
      "0.8 8 rmsprop True 10 433\n",
      "0.8 8 rmsprop False 434\n",
      "2018-12-05 14:15:03\n",
      "0.8 16 sgd True 435\n",
      "2018-12-05 14:15:33\n",
      "0.8 16 sgd True 3 435\n",
      "2018-12-05 14:16:31\n",
      "0.8 16 sgd True 5 436\n",
      "2018-12-05 14:17:28\n",
      "0.8 16 sgd True 8 437\n",
      "2018-12-05 14:18:26\n",
      "0.8 16 sgd True 10 438\n",
      "0.8 16 sgd False 439\n",
      "2018-12-05 14:19:22\n",
      "0.8 16 adam True 440\n",
      "2018-12-05 14:19:37\n",
      "0.8 16 adam True 3 440\n",
      "2018-12-05 14:20:36\n",
      "0.8 16 adam True 5 441\n",
      "2018-12-05 14:21:35\n",
      "0.8 16 adam True 8 442\n",
      "2018-12-05 14:22:34\n",
      "0.8 16 adam True 10 443\n",
      "0.8 16 adam False 444\n",
      "2018-12-05 14:23:32\n",
      "0.8 16 rmsprop True 445\n",
      "2018-12-05 14:23:53\n",
      "0.8 16 rmsprop True 3 445\n",
      "2018-12-05 14:24:51\n",
      "0.8 16 rmsprop True 5 446\n",
      "2018-12-05 14:25:49\n",
      "0.8 16 rmsprop True 8 447\n",
      "2018-12-05 14:26:47\n",
      "0.8 16 rmsprop True 10 448\n",
      "0.8 16 rmsprop False 449\n",
      "2018-12-05 14:27:44\n",
      "0.8 32 sgd True 450\n",
      "2018-12-05 14:28:00\n",
      "0.8 32 sgd True 3 450\n",
      "2018-12-05 14:28:29\n",
      "0.8 32 sgd True 5 451\n",
      "2018-12-05 14:28:58\n",
      "0.8 32 sgd True 8 452\n",
      "2018-12-05 14:29:28\n",
      "0.8 32 sgd True 10 453\n",
      "0.8 32 sgd False 454\n",
      "2018-12-05 14:29:56\n",
      "0.8 32 adam True 455\n",
      "2018-12-05 14:30:04\n",
      "0.8 32 adam True 3 455\n",
      "2018-12-05 14:30:35\n",
      "0.8 32 adam True 5 456\n",
      "2018-12-05 14:31:04\n",
      "0.8 32 adam True 8 457\n",
      "2018-12-05 14:31:34\n",
      "0.8 32 adam True 10 458\n",
      "0.8 32 adam False 459\n",
      "2018-12-05 14:32:04\n",
      "0.8 32 rmsprop True 460\n",
      "2018-12-05 14:32:16\n",
      "0.8 32 rmsprop True 3 460\n",
      "2018-12-05 14:32:45\n",
      "0.8 32 rmsprop True 5 461\n",
      "2018-12-05 14:33:15\n",
      "0.8 32 rmsprop True 8 462\n",
      "2018-12-05 14:33:44\n",
      "0.8 32 rmsprop True 10 463\n",
      "0.8 32 rmsprop False 464\n",
      "2018-12-05 14:34:13\n",
      "0.8 64 sgd True 465\n",
      "2018-12-05 14:34:23\n",
      "0.8 64 sgd True 3 465\n",
      "2018-12-05 14:34:38\n",
      "0.8 64 sgd True 5 466\n",
      "2018-12-05 14:34:53\n",
      "0.8 64 sgd True 8 467\n",
      "2018-12-05 14:35:08\n",
      "0.8 64 sgd True 10 468\n",
      "0.8 64 sgd False 469\n",
      "2018-12-05 14:35:23\n",
      "0.8 64 adam True 470\n",
      "2018-12-05 14:35:28\n",
      "0.8 64 adam True 3 470\n",
      "2018-12-05 14:35:44\n",
      "0.8 64 adam True 5 471\n",
      "2018-12-05 14:36:00\n",
      "0.8 64 adam True 8 472\n",
      "2018-12-05 14:36:16\n",
      "0.8 64 adam True 10 473\n",
      "0.8 64 adam False 474\n",
      "2018-12-05 14:36:31\n",
      "0.8 64 rmsprop True 475\n",
      "2018-12-05 14:36:38\n",
      "0.8 64 rmsprop True 3 475\n",
      "2018-12-05 14:36:53\n",
      "0.8 64 rmsprop True 5 476\n",
      "2018-12-05 14:37:09\n",
      "0.8 64 rmsprop True 8 477\n",
      "2018-12-05 14:37:24\n",
      "0.8 64 rmsprop True 10 478\n",
      "0.8 64 rmsprop False 479\n",
      "2018-12-05 14:37:39\n",
      "0.9 8 sgd True 480\n",
      "2018-12-05 14:37:45\n",
      "0.9 8 sgd True 3 480\n",
      "2018-12-05 14:39:40\n",
      "0.9 8 sgd True 5 481\n",
      "2018-12-05 14:41:34\n",
      "0.9 8 sgd True 8 482\n",
      "2018-12-05 14:43:27\n",
      "0.9 8 sgd True 10 483\n",
      "0.9 8 sgd False 484\n",
      "2018-12-05 14:45:19\n",
      "0.9 8 adam True 485\n",
      "2018-12-05 14:45:48\n",
      "0.9 8 adam True 3 485\n",
      "2018-12-05 14:47:46\n",
      "0.9 8 adam True 5 486\n",
      "2018-12-05 14:49:42\n",
      "0.9 8 adam True 8 487\n",
      "2018-12-05 14:51:39\n",
      "0.9 8 adam True 10 488\n",
      "0.9 8 adam False 489\n",
      "2018-12-05 14:53:34\n",
      "0.9 8 rmsprop True 490\n",
      "2018-12-05 14:54:16\n",
      "0.9 8 rmsprop True 3 490\n",
      "2018-12-05 14:56:13\n",
      "0.9 8 rmsprop True 5 491\n",
      "2018-12-05 14:58:07\n",
      "0.9 8 rmsprop True 8 492\n",
      "2018-12-05 15:00:01\n",
      "0.9 8 rmsprop True 10 493\n",
      "0.9 8 rmsprop False 494\n",
      "2018-12-05 15:01:54\n",
      "0.9 16 sgd True 495\n",
      "2018-12-05 15:02:24\n",
      "0.9 16 sgd True 3 495\n",
      "2018-12-05 15:03:22\n",
      "0.9 16 sgd True 5 496\n",
      "2018-12-05 15:04:19\n",
      "0.9 16 sgd True 8 497\n",
      "2018-12-05 15:05:16\n",
      "0.9 16 sgd True 10 498\n",
      "0.9 16 sgd False 499\n",
      "2018-12-05 15:06:13\n",
      "0.9 16 adam True 500\n",
      "2018-12-05 15:06:28\n",
      "0.9 16 adam True 3 500\n",
      "2018-12-05 15:07:27\n",
      "0.9 16 adam True 5 501\n",
      "2018-12-05 15:08:26\n",
      "0.9 16 adam True 8 502\n",
      "2018-12-05 15:09:24\n",
      "0.9 16 adam True 10 503\n",
      "0.9 16 adam False 504\n",
      "2018-12-05 15:10:22\n",
      "0.9 16 rmsprop True 505\n",
      "2018-12-05 15:10:43\n",
      "0.9 16 rmsprop True 3 505\n",
      "2018-12-05 15:11:42\n",
      "0.9 16 rmsprop True 5 506\n",
      "2018-12-05 15:12:39\n",
      "0.9 16 rmsprop True 8 507\n",
      "2018-12-05 15:13:37\n",
      "0.9 16 rmsprop True 10 508\n",
      "0.9 16 rmsprop False 509\n",
      "2018-12-05 15:14:34\n",
      "0.9 32 sgd True 510\n",
      "2018-12-05 15:14:50\n",
      "0.9 32 sgd True 3 510\n",
      "2018-12-05 15:15:19\n",
      "0.9 32 sgd True 5 511\n",
      "2018-12-05 15:15:48\n",
      "0.9 32 sgd True 8 512\n",
      "2018-12-05 15:16:17\n",
      "0.9 32 sgd True 10 513\n",
      "0.9 32 sgd False 514\n",
      "2018-12-05 15:16:46\n",
      "0.9 32 adam True 515\n",
      "2018-12-05 15:16:54\n",
      "0.9 32 adam True 3 515\n",
      "2018-12-05 15:17:24\n",
      "0.9 32 adam True 5 516\n",
      "2018-12-05 15:17:54\n",
      "0.9 32 adam True 8 517\n",
      "2018-12-05 15:18:24\n",
      "0.9 32 adam True 10 518\n",
      "0.9 32 adam False 519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-05 15:18:54\n",
      "0.9 32 rmsprop True 520\n",
      "2018-12-05 15:19:05\n",
      "0.9 32 rmsprop True 3 520\n",
      "2018-12-05 15:19:35\n",
      "0.9 32 rmsprop True 5 521\n",
      "2018-12-05 15:20:04\n",
      "0.9 32 rmsprop True 8 522\n",
      "2018-12-05 15:20:33\n",
      "0.9 32 rmsprop True 10 523\n",
      "0.9 32 rmsprop False 524\n",
      "2018-12-05 15:21:03\n",
      "0.9 64 sgd True 525\n",
      "2018-12-05 15:21:12\n",
      "0.9 64 sgd True 3 525\n",
      "2018-12-05 15:21:28\n",
      "0.9 64 sgd True 5 526\n",
      "2018-12-05 15:21:43\n",
      "0.9 64 sgd True 8 527\n",
      "2018-12-05 15:21:58\n",
      "0.9 64 sgd True 10 528\n",
      "0.9 64 sgd False 529\n",
      "2018-12-05 15:22:13\n",
      "0.9 64 adam True 530\n",
      "2018-12-05 15:22:18\n",
      "0.9 64 adam True 3 530\n",
      "2018-12-05 15:22:34\n",
      "0.9 64 adam True 5 531\n",
      "2018-12-05 15:22:50\n",
      "0.9 64 adam True 8 532\n",
      "2018-12-05 15:23:05\n",
      "0.9 64 adam True 10 533\n",
      "0.9 64 adam False 534\n",
      "2018-12-05 15:23:21\n",
      "0.9 64 rmsprop True 535\n",
      "2018-12-05 15:23:27\n",
      "0.9 64 rmsprop True 3 535\n",
      "2018-12-05 15:23:43\n",
      "0.9 64 rmsprop True 5 536\n",
      "2018-12-05 15:23:58\n",
      "0.9 64 rmsprop True 8 537\n",
      "2018-12-05 15:24:13\n",
      "0.9 64 rmsprop True 10 538\n",
      "0.9 64 rmsprop False 539\n",
      "2018-12-05 15:24:28\n"
     ]
    }
   ],
   "source": [
    "# LOGIST EXPERIMENT\n",
    "run = True\n",
    "\n",
    "# define hyperparameters\n",
    "#lr = 0.2\n",
    "#batch_size = 32\n",
    "epochs = 10\n",
    "#step = 300\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "c = 0\n",
    "\n",
    "if run:\n",
    "    for _lr in range(1, 10):\n",
    "        lr = (_lr)/10\n",
    "        for _batch_size in range(3, 8):\n",
    "            batch_size = 2**_batch_size\n",
    "            for optimizer in ['sgd', 'adam', 'rmsprop']:\n",
    "                for is_svrg in [True, False]:\n",
    "                    ctx = mx.cpu()\n",
    "                    #ctx = mx.gpu()\n",
    "                    print(lr, batch_size, optimizer, is_svrg, c)\n",
    "                    if is_svrg == False:\n",
    "                        logreg = LogisticRegression(\n",
    "                            dataset='MNIST', optimizer=optimizer, epochs=epochs, context=ctx, batch_size=batch_size, is_svrg=is_svrg,\n",
    "                            hyperparams={'learning_rate': lr})\n",
    "                        logreg.run_logistic(1)\n",
    "                        c = c + 1\n",
    "                        df = pd.concat([df, logreg.df_out])\n",
    "                    else:\n",
    "                        for update_freq in [3, 5, 8, 10]:\n",
    "                            logreg = LogisticRegression(\n",
    "                                dataset='MNIST', optimizer=optimizer, epochs=epochs, context=ctx, batch_size=batch_size, is_svrg=is_svrg,\n",
    "                            hyperparams={'learning_rate': lr})\n",
    "                            logreg.run_logistic(update_freq)\n",
    "                            print(lr, batch_size, optimizer, is_svrg, update_freq, c)\n",
    "                            c = c + 1\n",
    "                            \n",
    "                    \n",
    "                            df = pd.concat([df, logreg.df_out])\n",
    "                \n",
    "                            df.to_pickle('output.pkl')    \n",
    "                    #print(lr_svrg_sgd.df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        from svrg_optimization.svrg_module import SVRGModule\n",
    "    except ModuleNotFoundError:\n",
    "        download_svrg_module()\n",
    "        from svrg_optimization.svrg_module import SVRGModule\n",
    "\n",
    "    run = True\n",
    "\n",
    "    # define hyperparameters\n",
    "    lr = 0.2\n",
    "    batch_size = 32\n",
    "    epochs = 2\n",
    "    step = 300\n",
    "\n",
    "    if run:\n",
    "        ctx = mx.cpu()\n",
    "        lr_svrg_sgd = LogisticRegression(\n",
    "            dataset='MNIST', optimizer='sgd', epochs=epochs, context=ctx, batch_size=batch_size, is_svrg=True,\n",
    "            hyperparams={'learning_rate': 0.2})\n",
    "        lr_svrg_sgd.run_logistic(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mxnet_gpu)",
   "language": "python",
   "name": "mxnet_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
