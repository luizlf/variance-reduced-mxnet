{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import imp\n",
    "import logging\n",
    "imp.reload(logging)\n",
    "logging.basicConfig(filename=r'C:\\Users\\luiz\\Projects\\vradam-mxnet\\example.log',filemode='w',level=logging.DEBUG)\n",
    "logging.debug('This message should go to the log file')\n",
    "import mxnet as mx\n",
    "import requests as rq\n",
    "import pandas as pd\n",
    "import time, datetime\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_svrg_module():\n",
    "\n",
    "    try:\n",
    "        os.makedirs('./svrg_optimization')\n",
    "        init_svrg = rq.get(\n",
    "            'https://raw.githubusercontent.com/apache/incubator-mxnet/master/python/mxnet/contrib/svrg_optimization/__init__.py')\n",
    "        with open('./svrg_optimization/__init__.py', 'w') as file_init:\n",
    "            file_init.write(init_svrg.text)\n",
    "\n",
    "        modu_svrg = rq.get(\n",
    "            'https://raw.githubusercontent.com/apache/incubator-mxnet/master/python/mxnet/contrib/svrg_optimization/svrg_module.py')\n",
    "        with open('./svrg_optimization/svrg_module.py', 'w') as file_modu:\n",
    "            file_modu.write(modu_svrg.text)\n",
    "\n",
    "        opti_svrg = rq.get(\n",
    "            'https://raw.githubusercontent.com/apache/incubator-mxnet/master/python/mxnet/contrib/svrg_optimization/svrg_optimizer.py')\n",
    "        with open('./svrg_optimization/svrg_optimizer.py', 'w') as file_opti:\n",
    "            file_opti.write(opti_svrg.text)\n",
    "\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "\n",
    "    def __init__(self, dataset, epochs, context, batch_size, is_svrg=True, optimizer='sgd', hyperparams=None, **kwargs):\n",
    "        self.dataset = dataset.lower()\n",
    "        self.epochs = epochs\n",
    "        self.context = context\n",
    "        self.hyperparams = hyperparams\n",
    "        self.batch_size = batch_size\n",
    "        self.is_svrg = is_svrg\n",
    "        self.optimizer = optimizer\n",
    "        self.train_iter = None\n",
    "        self.test_iter = None\n",
    "\n",
    "        self.define_dataset()\n",
    "        self.log_path = r'evals.log'\n",
    "        import logging\n",
    "        imp.reload(logging)\n",
    "        logging.basicConfig(filename=self.log_path,filemode='w',level=logging.DEBUG)\n",
    "        #logging.debug('This message should go to the log file')\n",
    "        #self.logger = logging.getLogger()\n",
    "        #self.logger.setLevel(logging.DEBUG)  # logging to stdout\n",
    "\n",
    "        # return super().__init__(dataset, epochs, context, hyperparams, **kwargs)\n",
    "\n",
    "    def define_dataset(self):\n",
    "        if self.dataset == 'mnist' or self.dataset is None:\n",
    "            # define dataset and dataloader\n",
    "            mnist = mx.test_utils.get_mnist()\n",
    "\n",
    "            self.train_iter = mx.io.NDArrayIter(mnist['train_data'],\n",
    "                                                mnist['train_label'],\n",
    "                                                self.batch_size, shuffle=True, data_name='data',\n",
    "                                                label_name='softmax_label')\n",
    "            self.test_iter = mx.io.NDArrayIter(mnist['test_data'],\n",
    "                                               mnist['test_label'],\n",
    "                                               self.batch_size * 2, shuffle=False, data_name='data',\n",
    "                                               label_name='softmax_label')\n",
    "\n",
    "        elif self.dataset == 'cifar-10':\n",
    "            mx.test_utils.get_cifar10()\n",
    "\n",
    "    def process_log(self):\n",
    "        # define the paths to the training logs\n",
    "        logs = [\n",
    "            (0, self.log_path)\n",
    "        ]\n",
    "\n",
    "        # initialize the list of train rank-1 and rank-5 accuracies, along\n",
    "        # with the training loss\n",
    "        (trainRank1, trainRank5, trainLoss) = ([], [], [])\n",
    "\n",
    "        # initialize the list of validation rank-1 and rank-5 accuracies,\n",
    "        # along with the validation loss\n",
    "        (valRank1, valRank5, valLoss) = ([], [], [])\n",
    "\n",
    "        # loop over the training logs\n",
    "        for (i, (endEpoch, p)) in enumerate(logs):\n",
    "            # load the contents of the log file, then initialize the batch\n",
    "            # lists for the training and validation data\n",
    "            rows = open(p).read().strip()\n",
    "            (bTrainRank1, bTrainRank5, bTrainLoss) = ([], [], [])\n",
    "            (bValRank1, bValRank5, bValLoss) = ([], [], [])\n",
    "\n",
    "            # grab the set of training epochs\n",
    "            epochs = set(re.findall(r'Epoch\\[(\\d+)\\]', rows))\n",
    "            epochs = sorted([int(e) for e in epochs])\n",
    "\n",
    "            # loop over the epochs\n",
    "            for e in epochs:\n",
    "                # find all rank-1 accuracies, rank-5 accuracies, and loss\n",
    "                # values, then take the final entry in the list for each\n",
    "                s = r'Epoch\\[' + str(e) + '\\].*accuracy=([0]*\\.?[0-9]+)'\n",
    "\n",
    "                rank1 = re.findall(s, rows)[-2]\n",
    "                print(rank1)\n",
    "                #s = r'Epoch\\[' + str(e) + '\\].*top_k_accuracy_5=([0]*\\.?[0-9]+)'\n",
    "                #rank5 = re.findall(s, rows)[-2]\n",
    "                s = r'Epoch\\[' + str(e) + '\\].*cross-entropy=([0-9]*\\.?[0-9]+)'\n",
    "                loss = re.findall(s, rows)[-2]\n",
    "\n",
    "                # update the batch training lists\n",
    "                bTrainRank1.append(float(rank1))\n",
    "                #bTrainRank5.append(float(rank5))\n",
    "                bTrainLoss.append(float(loss))\n",
    "\n",
    "            # extract the validation rank-1 and rank-5 accuracies for each\n",
    "            # epoch, followed by the loss\n",
    "            bValRank1 = re.findall(r'Validation-accuracy=(.*)', rows)\n",
    "            #bValRank5 = re.findall(r'Validation-top_k_accuracy_5=(.*)', rows)\n",
    "            bValLoss = re.findall(r'Validation-cross-entropy=(.*)', rows)\n",
    "\n",
    "            # convert the validation rank-1, rank-5, and loss lists to floats\n",
    "            bValRank1 = [float(x) for x in bValRank1]\n",
    "            #bValRank5 = [float(x) for x in bValRank5]\n",
    "            bValLoss = [float(x) for x in bValLoss]\n",
    "            \n",
    "            df = pd.DataFrame({'val_acc': bValRank1,\n",
    "                               'val_loss': bValLoss,\n",
    "                               'train_acc': bTrainRank1,\n",
    "                               'train_loss': bTrainLoss})\n",
    "            \n",
    "            return df\n",
    "            \n",
    "    \n",
    "    def run_logistic(self, update_freq=2):\n",
    "\n",
    "        # data input and formatting\n",
    "        data = mx.sym.var('data')  # (bs, 1, 28, 28) - MNIST\n",
    "        label = mx.sym.var('softmax_label')\n",
    "        data = mx.sym.Flatten(data)  # (bs, 28*28) - MNIST\n",
    "\n",
    "        # logistic regression network\n",
    "        fc = mx.sym.FullyConnected(data, num_hidden=10, name='fc')\n",
    "        logist = mx.sym.SoftmaxOutput(fc, label=label, name='softmax')\n",
    "    \n",
    "        # metrics and eval\n",
    "        metric_list = [mx.metric.Accuracy(output_names=['softmax_output'], label_names=['softmax_label']),\n",
    "                       mx.metric.CrossEntropy(output_names=['softmax_output'], label_names=['softmax_label'])]\n",
    "        eval_metrics = mx.metric.CompositeEvalMetric(metric_list)\n",
    "\n",
    "        # create and 'compile' network\n",
    "        if self.is_svrg:\n",
    "            model = SVRGModule(symbol=logist,\n",
    "                               data_names=['data'],\n",
    "                               label_names=['softmax_label'],\n",
    "                               context=self.context,\n",
    "                               update_freq=update_freq)\n",
    "        else:\n",
    "            model = mx.mod.Module(logist,\n",
    "                                  data_names=['data'],\n",
    "                                  label_names=['softmax_label'],\n",
    "                                  context=self.context)\n",
    "\n",
    "        model.bind(data_shapes=self.train_iter.provide_data,\n",
    "                   label_shapes=self.train_iter.provide_label)\n",
    "        model.init_params()\n",
    "        model.init_optimizer(kvstore='local',\n",
    "                             optimizer=self.optimizer,\n",
    "                             optimizer_params=self.hyperparams)\n",
    "\n",
    "        timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        model.fit(self.train_iter,  # train data\n",
    "                  eval_data=self.test_iter,  # validation data\n",
    "                  # optimizer='sgd',  # use SGD to train\n",
    "                  # optimizer_params = (('learning_rate', lr),),  # use fixed learning rate\n",
    "                  eval_metric=eval_metrics,  # report accuracy during training\n",
    "                  # output progress for each 500 data batches\n",
    "                  batch_end_callback=mx.callback.Speedometer(batch_size, 1000),\n",
    "                  #epoch_end_callback=mx.callback.log_train_metric, #self.log_call,#mx.callback.do_checkpoint('logistic', 10),\n",
    "                  num_epoch=epochs)  # train for at most <epochs> dataset passes\n",
    "        \n",
    "        self.df_out = self.process_log()\n",
    "        print(timestamp)\n",
    "        self.df_out['timestamp'] = pd.Series(timestamp, index=self.df_out.index)\n",
    "        self.df_out['optimizer'] = pd.Series(self.optimizer, index=self.df_out.index)\n",
    "        self.df_out['is_svrg'] = pd.Series(self.is_svrg, index=self.df_out.index)\n",
    "        self.df_out['batch_size'] = pd.Series(self.batch_size, index=self.df_out.index)\n",
    "        self.df_out['hyperparams'] = pd.Series(str(self.hyperparams), index=self.df_out.index)\n",
    "        self.df_out['epoch'] = pd.Series(self.df_out.index + 1, index=self.df_out.index)\n",
    "        self.df_out['update_freq'] = pd.Series(update_freq, index=self.df_out.index)\n",
    "        \n",
    "        #print(self.df_out)\n",
    "        #val_score = model.score(self.train_iter, 'acc')[0][1]\n",
    "    \n",
    "        #train_score = model.score(self.train_iter, 'acc')[0][1]\n",
    "        #print(val_score, train_score)\n",
    "\n",
    "        # TODO: Define function returns\n",
    "        # IDEA: pandas df with all class params, timestamp, and scores. Save this to disk\n",
    "        # lr_params_svrg.update({lr: [val_score, train_score]})\n",
    "\n",
    "        #return train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from svrg_optimization.svrg_module import SVRGModule\n",
    "except ModuleNotFoundError:\n",
    "    download_svrg_module()\n",
    "    from svrg_optimization.svrg_module import SVRGModule\n",
    "\n",
    "run = True\n",
    "\n",
    "# define hyperparameters\n",
    "#lr = 0.2\n",
    "#batch_size = 32\n",
    "epochs = 10\n",
    "#step = 300\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "c = 0\n",
    "\n",
    "if run:\n",
    "    for _lr in range(1, 5):\n",
    "        lr = (_lr*2)/10\n",
    "        for _batch_size in range(3, 7):\n",
    "            batch_size = 2**_batch_size\n",
    "            for optimizer in ['sgd', 'adam']:\n",
    "                for is_svrg in [True, False]:\n",
    "                    ctx = mx.cpu()\n",
    "                    print(lr, batch_size, optimizer, is_svrg, c)\n",
    "                    if is_svrg == False:\n",
    "                        logreg = LogisticRegression(\n",
    "                            dataset='MNIST', optimizer=optimizer, epochs=epochs, context=ctx, batch_size=batch_size, is_svrg=is_svrg,\n",
    "                            hyperparams={'learning_rate': lr})\n",
    "                        logreg.run_logistic(1)\n",
    "                        c = c + 1\n",
    "                    else:\n",
    "                        for update_freq in [3, 5, 8]:\n",
    "                            logreg = LogisticRegression(\n",
    "                                dataset='MNIST', optimizer=optimizer, epochs=epochs, context=ctx, batch_size=batch_size, is_svrg=is_svrg,\n",
    "                            hyperparams={'learning_rate': lr})\n",
    "                            logreg.run_logistic(update_freq)\n",
    "                            print(lr, batch_size, optimizer, is_svrg, update_freq, c)\n",
    "                            c = c + 1\n",
    "                            \n",
    "                    \n",
    "                        df = pd.concat([df, logreg.df_out])\n",
    "                \n",
    "                df.to_pickle('output.pkl')    \n",
    "                    #print(lr_svrg_sgd.df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_svrg_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.890911\n",
      "0.898491\n",
      "2018-12-04 00:31:16\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.877090  0.405829   0.890911     0.37635  2018-12-04 00:31:16       sgd   \n",
      "1  0.883758  0.395952   0.898491     0.35629  2018-12-04 00:31:16       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  epoch  update_freq  \n",
      "0     True          32  {'learning_rate': 0.2}      1            2  \n",
      "1     True          32  {'learning_rate': 0.2}      2            2  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        from svrg_optimization.svrg_module import SVRGModule\n",
    "    except ModuleNotFoundError:\n",
    "        download_svrg_module()\n",
    "        from svrg_optimization.svrg_module import SVRGModule\n",
    "\n",
    "    run = True\n",
    "\n",
    "    # define hyperparameters\n",
    "    lr = 0.2\n",
    "    batch_size = 32\n",
    "    epochs = 2\n",
    "    step = 300\n",
    "\n",
    "    if run:\n",
    "        ctx = mx.cpu()\n",
    "        lr_svrg_sgd = LogisticRegression(\n",
    "            dataset='MNIST', optimizer='sgd', epochs=epochs, context=ctx, batch_size=batch_size, is_svrg=True,\n",
    "            hyperparams={'learning_rate': 0.2})\n",
    "        lr_svrg_sgd.run_logistic(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.concat([dd, lr_svrg_sgd.df_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.to_pickle('out.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>is_svrg</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.904061</td>\n",
       "      <td>0.348061</td>\n",
       "      <td>0.889266</td>\n",
       "      <td>0.381538</td>\n",
       "      <td>2018-12-04 00:21:46</td>\n",
       "      <td>sgd</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>{'learning_rate': 0.2}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.906947</td>\n",
       "      <td>0.348599</td>\n",
       "      <td>0.897955</td>\n",
       "      <td>0.361218</td>\n",
       "      <td>2018-12-04 00:21:46</td>\n",
       "      <td>sgd</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>{'learning_rate': 0.2}</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.904061</td>\n",
       "      <td>0.348061</td>\n",
       "      <td>0.889266</td>\n",
       "      <td>0.381538</td>\n",
       "      <td>2018-12-04 00:21:46</td>\n",
       "      <td>sgd</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>{'learning_rate': 0.2}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.906947</td>\n",
       "      <td>0.348599</td>\n",
       "      <td>0.897955</td>\n",
       "      <td>0.361218</td>\n",
       "      <td>2018-12-04 00:21:46</td>\n",
       "      <td>sgd</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>{'learning_rate': 0.2}</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
       "0  0.904061  0.348061   0.889266    0.381538  2018-12-04 00:21:46       sgd   \n",
       "1  0.906947  0.348599   0.897955    0.361218  2018-12-04 00:21:46       sgd   \n",
       "0  0.904061  0.348061   0.889266    0.381538  2018-12-04 00:21:46       sgd   \n",
       "1  0.906947  0.348599   0.897955    0.361218  2018-12-04 00:21:46       sgd   \n",
       "\n",
       "   is_svrg  batch_size             hyperparams  epochs  \n",
       "0     True          32  {'learning_rate': 0.2}       1  \n",
       "1     True          32  {'learning_rate': 0.2}       2  \n",
       "0     True          32  {'learning_rate': 0.2}       1  \n",
       "1     True          32  {'learning_rate': 0.2}       2  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle('out.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_svrg_sgd.df_out['timestamp'] = pd.Series('aa', index=lr_svrg_sgd.df_out.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>is_svrg</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>hyperparams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.894307</td>\n",
       "      <td>0.367086</td>\n",
       "      <td>0.893549</td>\n",
       "      <td>0.371103</td>\n",
       "      <td>2018-12-03 23:53:46</td>\n",
       "      <td>sgd</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>{'learning_rate': 0.2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.896696</td>\n",
       "      <td>0.365647</td>\n",
       "      <td>0.902072</td>\n",
       "      <td>0.352716</td>\n",
       "      <td>2018-12-03 23:53:46</td>\n",
       "      <td>sgd</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>{'learning_rate': 0.2}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
       "0  0.894307  0.367086   0.893549    0.371103  2018-12-03 23:53:46       sgd   \n",
       "1  0.896696  0.365647   0.902072    0.352716  2018-12-03 23:53:46       sgd   \n",
       "\n",
       "   is_svrg  batch_size             hyperparams  \n",
       "0     True          32  {'learning_rate': 0.2}  \n",
       "1     True          32  {'learning_rate': 0.2}  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_svrg_sgd.df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.2}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_svrg_sgd.hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE\n",
    "# python plot_log.py --network VGGNet --dataset ImageNet\n",
    "\n",
    "# import the necessary packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the argument parse and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-n\", \"--network\", required=True,\n",
    "    help=\"name of network\")\n",
    "ap.add_argument(\"-d\", \"--dataset\", required=True,\n",
    "    help=\"name of dataset\")\n",
    "args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.888954\n",
      "0.893549\n",
      "0.919786\n"
     ]
    }
   ],
   "source": [
    "# define the paths to the training logs\n",
    "logs = [\n",
    "    (0, \"example2.log\")\n",
    "]\n",
    "\n",
    "# initialize the list of train rank-1 and rank-5 accuracies, along\n",
    "# with the training loss\n",
    "(trainRank1, trainRank5, trainLoss) = ([], [], [])\n",
    "\n",
    "# initialize the list of validation rank-1 and rank-5 accuracies,\n",
    "# along with the validation loss\n",
    "(valRank1, valRank5, valLoss) = ([], [], [])\n",
    "\n",
    "# loop over the training logs\n",
    "for (i, (endEpoch, p)) in enumerate(logs):\n",
    "    # load the contents of the log file, then initialize the batch\n",
    "    # lists for the training and validation data\n",
    "    rows = open(p).read().strip()\n",
    "    (bTrainRank1, bTrainRank5, bTrainLoss) = ([], [], [])\n",
    "    (bValRank1, bValRank5, bValLoss) = ([], [], [])\n",
    "\n",
    "    # grab the set of training epochs\n",
    "    epochs = set(re.findall(r'Epoch\\[(\\d+)\\]', rows))\n",
    "    epochs = sorted([int(e) for e in epochs])\n",
    "\n",
    "    # loop over the epochs\n",
    "    for e in epochs:\n",
    "        # find all rank-1 accuracies, rank-5 accuracies, and loss\n",
    "        # values, then take the final entry in the list for each\n",
    "        s = r'Epoch\\[' + str(e) + '\\].*accuracy=([0]*\\.?[0-9]+)'\n",
    "        \n",
    "        rank1 = re.findall(s, rows)[-2]\n",
    "        print(rank1)\n",
    "        #s = r'Epoch\\[' + str(e) + '\\].*top_k_accuracy_5=([0]*\\.?[0-9]+)'\n",
    "        #rank5 = re.findall(s, rows)[-2]\n",
    "        s = r'Epoch\\[' + str(e) + '\\].*cross-entropy=([0-9]*\\.?[0-9]+)'\n",
    "        loss = re.findall(s, rows)[-2]\n",
    "\n",
    "        # update the batch training lists\n",
    "        bTrainRank1.append(float(rank1))\n",
    "        #bTrainRank5.append(float(rank5))\n",
    "        bTrainLoss.append(float(loss))\n",
    "    \n",
    "    # extract the validation rank-1 and rank-5 accuracies for each\n",
    "    # epoch, followed by the loss\n",
    "    bValRank1 = re.findall(r'Validation-accuracy=(.*)', rows)\n",
    "    #bValRank5 = re.findall(r'Validation-top_k_accuracy_5=(.*)', rows)\n",
    "    bValLoss = re.findall(r'Validation-cross-entropy=(.*)', rows)\n",
    "\n",
    "    # convert the validation rank-1, rank-5, and loss lists to floats\n",
    "    bValRank1 = [float(x) for x in bValRank1]\n",
    "    #bValRank5 = [float(x) for x in bValRank5]\n",
    "    bValLoss = [float(x) for x in bValLoss]\n",
    "\n",
    "    # check to see if we are examining a log file other than the\n",
    "    # first one, and if so, use the number of the final epoch in\n",
    "    # the log file as our slice index\n",
    "    if i > 0 and endEpoch is not None:\n",
    "        trainEnd = endEpoch - logs[i - 1][0]\n",
    "        valEnd = endEpoch - logs[i - 1][0]\n",
    "\n",
    "    # otherwise, this is the first epoch so no subtraction needs\n",
    "    # to be done\n",
    "    else:\n",
    "        trainEnd = endEpoch\n",
    "        valEnd = endEpoch\n",
    "\n",
    "    # update the training lists\n",
    "    trainRank1.extend(bTrainRank1[0:trainEnd])\n",
    "    #trainRank5.extend(bTrainRank5[0:trainEnd])\n",
    "    trainLoss.extend(bTrainLoss[0:trainEnd])\n",
    "\n",
    "    # update the validation lists\n",
    "    valRank1.extend(bValRank1[0:valEnd])\n",
    "    #valRank5.extend(bValRank5[0:valEnd])\n",
    "    valLoss.extend(bValLoss[0:valEnd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.388628, 0.372752, 0.292123]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bTrainLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracies\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, len(trainRank1)), trainRank1,\n",
    "    label=\"train_rank1\")\n",
    "plt.plot(np.arange(0, len(trainRank5)), trainRank5,\n",
    "    label=\"train_rank5\")\n",
    "plt.plot(np.arange(0, len(valRank1)), valRank1,\n",
    "    label=\"val_rank1\")\n",
    "plt.plot(np.arange(0, len(valRank5)), valRank5,\n",
    "    label=\"val_rank5\")\n",
    "plt.title(\"{}: rank-1 and rank-5 accuracy on {}\".format(\n",
    "    args[\"network\"], args[\"dataset\"]))\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# plot the losses\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, len(trainLoss)), trainLoss,\n",
    "    label=\"train_loss\")\n",
    "plt.plot(np.arange(0, len(valLoss)), valLoss,\n",
    "    label=\"val_loss\")\n",
    "plt.title(\"{}: cross-entropy loss on {}\".format(args[\"network\"],\n",
    "    args[\"dataset\"]))\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mxnet_gpu)",
   "language": "python",
   "name": "mxnet_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
