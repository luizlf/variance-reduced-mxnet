{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import imp\n",
    "import logging\n",
    "imp.reload(logging)\n",
    "logging.basicConfig(filename=r'C:\\Users\\luiz\\Projects\\vradam-mxnet\\example.log',filemode='w',level=logging.DEBUG)\n",
    "logging.debug('This message should go to the log file')\n",
    "import mxnet as mx\n",
    "import requests as rq\n",
    "import pandas as pd\n",
    "import time, datetime\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_svrg_module():\n",
    "\n",
    "    try:\n",
    "        os.makedirs('./svrg_optimization')\n",
    "        init_svrg = rq.get(\n",
    "            'https://raw.githubusercontent.com/apache/incubator-mxnet/master/python/mxnet/contrib/svrg_optimization/__init__.py')\n",
    "        with open('./svrg_optimization/__init__.py', 'w') as file_init:\n",
    "            file_init.write(init_svrg.text)\n",
    "\n",
    "        modu_svrg = rq.get(\n",
    "            'https://raw.githubusercontent.com/apache/incubator-mxnet/master/python/mxnet/contrib/svrg_optimization/svrg_module.py')\n",
    "        with open('./svrg_optimization/svrg_module.py', 'w') as file_modu:\n",
    "            file_modu.write(modu_svrg.text)\n",
    "\n",
    "        opti_svrg = rq.get(\n",
    "            'https://raw.githubusercontent.com/apache/incubator-mxnet/master/python/mxnet/contrib/svrg_optimization/svrg_optimizer.py')\n",
    "        with open('./svrg_optimization/svrg_optimizer.py', 'w') as file_opti:\n",
    "            file_opti.write(opti_svrg.text)\n",
    "\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "\n",
    "    def __init__(self, dataset, epochs, context, batch_size, is_svrg=True, optimizer='sgd', hyperparams=None, **kwargs):\n",
    "        self.dataset = dataset.lower()\n",
    "        self.epochs = epochs\n",
    "        self.context = context\n",
    "        self.hyperparams = hyperparams\n",
    "        self.batch_size = batch_size\n",
    "        self.is_svrg = is_svrg\n",
    "        self.optimizer = optimizer\n",
    "        self.train_iter = None\n",
    "        self.test_iter = None\n",
    "\n",
    "        self.define_dataset()\n",
    "        self.log_path = r'evals.log'\n",
    "        import logging\n",
    "        imp.reload(logging)\n",
    "        logging.basicConfig(filename=self.log_path,filemode='w',level=logging.DEBUG)\n",
    "        #logging.debug('This message should go to the log file')\n",
    "        #self.logger = logging.getLogger()\n",
    "        #self.logger.setLevel(logging.DEBUG)  # logging to stdout\n",
    "\n",
    "        # return super().__init__(dataset, epochs, context, hyperparams, **kwargs)\n",
    "\n",
    "    def define_dataset(self):\n",
    "        if self.dataset == 'mnist' or self.dataset is None:\n",
    "            # define dataset and dataloader\n",
    "            mnist = mx.test_utils.get_mnist()\n",
    "\n",
    "            self.train_iter = mx.io.NDArrayIter(mnist['train_data'],\n",
    "                                                mnist['train_label'],\n",
    "                                                self.batch_size, shuffle=True, data_name='data',\n",
    "                                                label_name='softmax_label')\n",
    "            self.test_iter = mx.io.NDArrayIter(mnist['test_data'],\n",
    "                                               mnist['test_label'],\n",
    "                                               self.batch_size * 2, shuffle=False, data_name='data',\n",
    "                                               label_name='softmax_label')\n",
    "\n",
    "        elif self.dataset == 'cifar-10':\n",
    "            mx.test_utils.get_cifar10()\n",
    "\n",
    "    def process_log(self):\n",
    "        # define the paths to the training logs\n",
    "        logs = [\n",
    "            (0, self.log_path)\n",
    "        ]\n",
    "\n",
    "        # initialize the list of train rank-1 and rank-5 accuracies, along\n",
    "        # with the training loss\n",
    "        (trainRank1, trainRank5, trainLoss) = ([], [], [])\n",
    "\n",
    "        # initialize the list of validation rank-1 and rank-5 accuracies,\n",
    "        # along with the validation loss\n",
    "        (valRank1, valRank5, valLoss) = ([], [], [])\n",
    "\n",
    "        # loop over the training logs\n",
    "        for (i, (endEpoch, p)) in enumerate(logs):\n",
    "            # load the contents of the log file, then initialize the batch\n",
    "            # lists for the training and validation data\n",
    "            rows = open(p).read().strip()\n",
    "            (bTrainRank1, bTrainRank5, bTrainLoss) = ([], [], [])\n",
    "            (bValRank1, bValRank5, bValLoss) = ([], [], [])\n",
    "\n",
    "            # grab the set of training epochs\n",
    "            epochs = set(re.findall(r'Epoch\\[(\\d+)\\]', rows))\n",
    "            epochs = sorted([int(e) for e in epochs])\n",
    "\n",
    "            # loop over the epochs\n",
    "            for e in epochs:\n",
    "                # find all rank-1 accuracies, rank-5 accuracies, and loss\n",
    "                # values, then take the final entry in the list for each\n",
    "                s = r'Epoch\\[' + str(e) + '\\].*accuracy=([0]*\\.?[0-9]+)'\n",
    "\n",
    "                rank1 = re.findall(s, rows)[-2]\n",
    "                print(rank1)\n",
    "                #s = r'Epoch\\[' + str(e) + '\\].*top_k_accuracy_5=([0]*\\.?[0-9]+)'\n",
    "                #rank5 = re.findall(s, rows)[-2]\n",
    "                s = r'Epoch\\[' + str(e) + '\\].*cross-entropy=([0-9]*\\.?[0-9]+)'\n",
    "                loss = re.findall(s, rows)[-2]\n",
    "\n",
    "                # update the batch training lists\n",
    "                bTrainRank1.append(float(rank1))\n",
    "                #bTrainRank5.append(float(rank5))\n",
    "                bTrainLoss.append(float(loss))\n",
    "\n",
    "            # extract the validation rank-1 and rank-5 accuracies for each\n",
    "            # epoch, followed by the loss\n",
    "            bValRank1 = re.findall(r'Validation-accuracy=(.*)', rows)\n",
    "            #bValRank5 = re.findall(r'Validation-top_k_accuracy_5=(.*)', rows)\n",
    "            bValLoss = re.findall(r'Validation-cross-entropy=(.*)', rows)\n",
    "\n",
    "            # convert the validation rank-1, rank-5, and loss lists to floats\n",
    "            bValRank1 = [float(x) for x in bValRank1]\n",
    "            #bValRank5 = [float(x) for x in bValRank5]\n",
    "            bValLoss = [float(x) for x in bValLoss]\n",
    "            \n",
    "            df = pd.DataFrame({'val_acc': bValRank1,\n",
    "                               'val_loss': bValLoss,\n",
    "                               'train_acc': bTrainRank1,\n",
    "                               'train_loss': bTrainLoss})\n",
    "            \n",
    "            return df\n",
    "            \n",
    "    \n",
    "    def run_logistic(self, update_freq=2):\n",
    "\n",
    "        # data input and formatting\n",
    "        data = mx.sym.var('data')  # (bs, 1, 28, 28) - MNIST\n",
    "        label = mx.sym.var('softmax_label')\n",
    "        data = mx.sym.Flatten(data)  # (bs, 28*28) - MNIST\n",
    "\n",
    "        # logistic regression network\n",
    "        fc = mx.sym.FullyConnected(data, num_hidden=10, name='fc')\n",
    "        logist = mx.sym.SoftmaxOutput(fc, label=label, name='softmax')\n",
    "    \n",
    "        # metrics and eval\n",
    "        metric_list = [mx.metric.Accuracy(output_names=['softmax_output'], label_names=['softmax_label']),\n",
    "                       mx.metric.CrossEntropy(output_names=['softmax_output'], label_names=['softmax_label'])]\n",
    "        eval_metrics = mx.metric.CompositeEvalMetric(metric_list)\n",
    "\n",
    "        # create and 'compile' network\n",
    "        if self.is_svrg:\n",
    "            model = SVRGModule(symbol=logist,\n",
    "                               data_names=['data'],\n",
    "                               label_names=['softmax_label'],\n",
    "                               context=self.context,\n",
    "                               update_freq=update_freq)\n",
    "        else:\n",
    "            model = mx.mod.Module(logist,\n",
    "                                  data_names=['data'],\n",
    "                                  label_names=['softmax_label'],\n",
    "                                  context=self.context)\n",
    "\n",
    "        model.bind(data_shapes=self.train_iter.provide_data,\n",
    "                   label_shapes=self.train_iter.provide_label)\n",
    "        model.init_params()\n",
    "        model.init_optimizer(kvstore='local',\n",
    "                             optimizer=self.optimizer,\n",
    "                             optimizer_params=self.hyperparams)\n",
    "\n",
    "        timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        model.fit(self.train_iter,  # train data\n",
    "                  eval_data=self.test_iter,  # validation data\n",
    "                  # optimizer='sgd',  # use SGD to train\n",
    "                  # optimizer_params = (('learning_rate', lr),),  # use fixed learning rate\n",
    "                  eval_metric=eval_metrics,  # report accuracy during training\n",
    "                  # output progress for each 500 data batches\n",
    "                  batch_end_callback=mx.callback.Speedometer(batch_size, 1000),\n",
    "                  #epoch_end_callback=mx.callback.log_train_metric, #self.log_call,#mx.callback.do_checkpoint('logistic', 10),\n",
    "                  num_epoch=epochs)  # train for at most <epochs> dataset passes\n",
    "        \n",
    "        self.df_out = self.process_log()\n",
    "        print(timestamp)\n",
    "        self.df_out['timestamp'] = pd.Series(timestamp, index=self.df_out.index)\n",
    "        self.df_out['optimizer'] = pd.Series(self.optimizer, index=self.df_out.index)\n",
    "        self.df_out['is_svrg'] = pd.Series(self.is_svrg, index=self.df_out.index)\n",
    "        self.df_out['batch_size'] = pd.Series(self.batch_size, index=self.df_out.index)\n",
    "        self.df_out['hyperparams'] = pd.Series(str(self.hyperparams), index=self.df_out.index)\n",
    "        self.df_out['epoch'] = pd.Series(self.df_out.index + 1, index=self.df_out.index)\n",
    "        \n",
    "        print(self.df_out)\n",
    "        #val_score = model.score(self.train_iter, 'acc')[0][1]\n",
    "    \n",
    "        #train_score = model.score(self.train_iter, 'acc')[0][1]\n",
    "        #print(val_score, train_score)\n",
    "\n",
    "        # TODO: Define function returns\n",
    "        # IDEA: pandas df with all class params, timestamp, and scores. Save this to disk\n",
    "        # lr_params_svrg.update({lr: [val_score, train_score]})\n",
    "\n",
    "        #return train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 8 sgd True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luiz\\Projects\\vradam-mxnet\\svrg_optimization\\svrg_module.py:476: UserWarning: Parameters already initialized and force_init=False. init_params call ignored.\n",
      "  allow_missing=allow_missing, force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.878257\n",
      "0.887024\n",
      "2018-12-03 23:59:49\n",
      "   val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0   0.8708  0.493704   0.878257    0.489682  2018-12-03 23:59:49       sgd   \n",
      "1   0.8740  0.510832   0.887024    0.482395  2018-12-03 23:59:49       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True           8  {'learning_rate': 0.1}  \n",
      "1     True           8  {'learning_rate': 0.1}  \n",
      "0.87575 0.87575\n",
      "   val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0   0.8708  0.493704   0.878257    0.489682  2018-12-03 23:59:49       sgd   \n",
      "1   0.8740  0.510832   0.887024    0.482395  2018-12-03 23:59:49       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True           8  {'learning_rate': 0.1}  \n",
      "1     True           8  {'learning_rate': 0.1}  \n",
      "0.1 8 sgd False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luiz\\AppData\\Roaming\\Python\\Python36\\site-packages\\mxnet\\module\\base_module.py:502: UserWarning: Parameters already initialized and force_init=False. init_params call ignored.\n",
      "  allow_missing=allow_missing, force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.914830\n",
      "0.918086\n",
      "2018-12-04 00:00:16\n",
      "   val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0   0.9137  0.306695   0.914830    0.324313  2018-12-04 00:00:16       sgd   \n",
      "1   0.9162  0.298178   0.918086    0.305179  2018-12-04 00:00:16       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False           8  {'learning_rate': 0.1}  \n",
      "1    False           8  {'learning_rate': 0.1}  \n",
      "0.9187833333333333 0.9187833333333333\n",
      "   val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0   0.9137  0.306695   0.914830    0.324313  2018-12-04 00:00:16       sgd   \n",
      "1   0.9162  0.298178   0.918086    0.305179  2018-12-04 00:00:16       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False           8  {'learning_rate': 0.1}  \n",
      "1    False           8  {'learning_rate': 0.1}  \n",
      "0.1 8 adam True\n",
      "0.882766\n",
      "0.892034\n",
      "2018-12-04 00:00:25\n",
      "   val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0   0.8149  3.597401   0.882766    2.336789  2018-12-04 00:00:25      adam   \n",
      "1   0.8362  3.318359   0.892034    2.192310  2018-12-04 00:00:25      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True           8  {'learning_rate': 0.1}  \n",
      "1     True           8  {'learning_rate': 0.1}  \n",
      "0.8367833333333333 0.8367833333333333\n",
      "   val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0   0.8149  3.597401   0.882766    2.336789  2018-12-04 00:00:25      adam   \n",
      "1   0.8362  3.318359   0.892034    2.192310  2018-12-04 00:00:25      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True           8  {'learning_rate': 0.1}  \n",
      "1     True           8  {'learning_rate': 0.1}  \n",
      "0.1 8 adam False\n",
      "0.873497\n",
      "0.881513\n",
      "2018-12-04 00:00:53\n",
      "   val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0   0.8997  1.817576   0.873497    2.330513  2018-12-04 00:00:53      adam   \n",
      "1   0.9037  1.857088   0.881513    2.273300  2018-12-04 00:00:53      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False           8  {'learning_rate': 0.1}  \n",
      "1    False           8  {'learning_rate': 0.1}  \n",
      "0.9033166666666667 0.9033166666666667\n",
      "   val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0   0.8997  1.817576   0.873497    2.330513  2018-12-04 00:00:53      adam   \n",
      "1   0.9037  1.857088   0.881513    2.273300  2018-12-04 00:00:53      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False           8  {'learning_rate': 0.1}  \n",
      "1    False           8  {'learning_rate': 0.1}  \n",
      "0.1 8 adagrad True\n",
      "0.910571\n",
      "0.916333\n",
      "2018-12-04 00:01:05\n",
      "   val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0   0.9188  0.305809   0.910571    0.334326  2018-12-04 00:01:05   adagrad   \n",
      "1   0.9229  0.289289   0.916333    0.307913  2018-12-04 00:01:05   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True           8  {'learning_rate': 0.1}  \n",
      "1     True           8  {'learning_rate': 0.1}  \n",
      "0.9220666666666667 0.9220666666666667\n",
      "   val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0   0.9188  0.305809   0.910571    0.334326  2018-12-04 00:01:05   adagrad   \n",
      "1   0.9229  0.289289   0.916333    0.307913  2018-12-04 00:01:05   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True           8  {'learning_rate': 0.1}  \n",
      "1     True           8  {'learning_rate': 0.1}  \n",
      "0.1 8 adagrad False\n",
      "0.915080\n",
      "0.921092\n",
      "2018-12-04 00:01:39\n",
      "   val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0   0.9232  0.274757   0.915080    0.279127  2018-12-04 00:01:39   adagrad   \n",
      "1   0.9251  0.269383   0.921092    0.260645  2018-12-04 00:01:39   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False           8  {'learning_rate': 0.1}  \n",
      "1    False           8  {'learning_rate': 0.1}  \n",
      "0.9287 0.9287\n",
      "   val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0   0.9232  0.274757   0.915080    0.279127  2018-12-04 00:01:39   adagrad   \n",
      "1   0.9251  0.269383   0.921092    0.260645  2018-12-04 00:01:39   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False           8  {'learning_rate': 0.1}  \n",
      "1    False           8  {'learning_rate': 0.1}  \n",
      "0.1 16 sgd True\n",
      "0.899849\n",
      "0.905371\n",
      "2018-12-04 00:01:54\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.886382  0.379005   0.899849    0.372861  2018-12-04 00:01:54       sgd   \n",
      "1  0.893371  0.368906   0.905371    0.356993  2018-12-04 00:01:54       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True          16  {'learning_rate': 0.1}  \n",
      "1     True          16  {'learning_rate': 0.1}  \n",
      "0.8947666666666667 0.8947666666666667\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.886382  0.379005   0.899849    0.372861  2018-12-04 00:01:54       sgd   \n",
      "1  0.893371  0.368906   0.905371    0.356993  2018-12-04 00:01:54       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True          16  {'learning_rate': 0.1}  \n",
      "1     True          16  {'learning_rate': 0.1}  \n",
      "0.1 16 sgd False\n",
      "0.905622\n",
      "0.911647\n",
      "2018-12-04 00:02:08\n",
      "   val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.91863  0.293450   0.905622    0.325232  2018-12-04 00:02:08       sgd   \n",
      "1  0.91903  0.282198   0.911647    0.302720  2018-12-04 00:02:08       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False          16  {'learning_rate': 0.1}  \n",
      "1    False          16  {'learning_rate': 0.1}  \n",
      "0.92125 0.92125\n",
      "   val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.91863  0.293450   0.905622    0.325232  2018-12-04 00:02:08       sgd   \n",
      "1  0.91903  0.282198   0.911647    0.302720  2018-12-04 00:02:08       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False          16  {'learning_rate': 0.1}  \n",
      "1    False          16  {'learning_rate': 0.1}  \n",
      "0.1 16 adam True\n",
      "0.871737\n",
      "0.878514\n",
      "2018-12-04 00:02:13\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.870907  2.302772   0.871737    2.382346  2018-12-04 00:02:13      adam   \n",
      "1  0.874101  2.357547   0.878514    2.249331  2018-12-04 00:02:13      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True          16  {'learning_rate': 0.1}  \n",
      "1     True          16  {'learning_rate': 0.1}  \n",
      "0.8741 0.8741\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.870907  2.302772   0.871737    2.382346  2018-12-04 00:02:13      adam   \n",
      "1  0.874101  2.357547   0.878514    2.249331  2018-12-04 00:02:13      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True          16  {'learning_rate': 0.1}  \n",
      "1     True          16  {'learning_rate': 0.1}  \n",
      "0.1 16 adam False\n",
      "0.874247\n",
      "0.881777\n",
      "2018-12-04 00:02:28\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.888179  1.710361   0.874247    1.892247  2018-12-04 00:02:28      adam   \n",
      "1  0.878994  1.998402   0.881777    1.884780  2018-12-04 00:02:28      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False          16  {'learning_rate': 0.1}  \n",
      "1    False          16  {'learning_rate': 0.1}  \n",
      "0.8774166666666666 0.8774166666666666\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.888179  1.710361   0.874247    1.892247  2018-12-04 00:02:28      adam   \n",
      "1  0.878994  1.998402   0.881777    1.884780  2018-12-04 00:02:28      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False          16  {'learning_rate': 0.1}  \n",
      "1    False          16  {'learning_rate': 0.1}  \n",
      "0.1 16 adagrad True\n",
      "0.914910\n",
      "0.921436\n",
      "2018-12-04 00:02:34\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.915635  0.313054   0.914910    0.307319  2018-12-04 00:02:34   adagrad   \n",
      "1  0.920427  0.295060   0.921436    0.284012  2018-12-04 00:02:34   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True          16  {'learning_rate': 0.1}  \n",
      "1     True          16  {'learning_rate': 0.1}  \n",
      "0.9233666666666667 0.9233666666666667\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.915635  0.313054   0.914910    0.307319  2018-12-04 00:02:34   adagrad   \n",
      "1  0.920427  0.295060   0.921436    0.284012  2018-12-04 00:02:34   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True          16  {'learning_rate': 0.1}  \n",
      "1     True          16  {'learning_rate': 0.1}  \n",
      "0.1 16 adagrad False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.925954\n",
      "0.931476\n",
      "2018-12-04 00:02:52\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.922324  0.279711   0.925954    0.289689  2018-12-04 00:02:52   adagrad   \n",
      "1  0.925819  0.271609   0.931476    0.270806  2018-12-04 00:02:52   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False          16  {'learning_rate': 0.1}  \n",
      "1    False          16  {'learning_rate': 0.1}  \n",
      "0.9285666666666667 0.9285666666666667\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.922324  0.279711   0.925954    0.289689  2018-12-04 00:02:52   adagrad   \n",
      "1  0.925819  0.271609   0.931476    0.270806  2018-12-04 00:02:52   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False          16  {'learning_rate': 0.1}  \n",
      "1    False          16  {'learning_rate': 0.1}  \n",
      "0.1 32 sgd True\n",
      "0.897727\n",
      "0.907754\n",
      "2018-12-04 00:03:00\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.914013  0.314057   0.897727    0.354652  2018-12-04 00:03:00       sgd   \n",
      "1  0.916202  0.298460   0.907754    0.327692  2018-12-04 00:03:00       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True          32  {'learning_rate': 0.1}  \n",
      "1     True          32  {'learning_rate': 0.1}  \n",
      "0.9125 0.9125\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.914013  0.314057   0.897727    0.354652  2018-12-04 00:03:00       sgd   \n",
      "1  0.916202  0.298460   0.907754    0.327692  2018-12-04 00:03:00       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True          32  {'learning_rate': 0.1}  \n",
      "1     True          32  {'learning_rate': 0.1}  \n",
      "0.1 32 sgd False\n",
      "0.907336\n",
      "0.914522\n",
      "2018-12-04 00:03:07\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.915506  0.307777   0.907336    0.334868  2018-12-04 00:03:07       sgd   \n",
      "1  0.921178  0.288709   0.914522    0.306138  2018-12-04 00:03:07       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False          32  {'learning_rate': 0.1}  \n",
      "1    False          32  {'learning_rate': 0.1}  \n",
      "0.91785 0.91785\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.915506  0.307777   0.907336    0.334868  2018-12-04 00:03:07       sgd   \n",
      "1  0.921178  0.288709   0.914522    0.306138  2018-12-04 00:03:07       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False          32  {'learning_rate': 0.1}  \n",
      "1    False          32  {'learning_rate': 0.1}  \n",
      "0.1 32 adam True\n",
      "0.859124\n",
      "0.870906\n",
      "2018-12-04 00:03:11\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.885748  1.725338   0.859124    2.117738  2018-12-04 00:03:11      adam   \n",
      "1  0.895502  1.692403   0.870906    2.029100  2018-12-04 00:03:11      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True          32  {'learning_rate': 0.1}  \n",
      "1     True          32  {'learning_rate': 0.1}  \n",
      "0.8942666666666667 0.8942666666666667\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.885748  1.725338   0.859124    2.117738  2018-12-04 00:03:11      adam   \n",
      "1  0.895502  1.692403   0.870906    2.029100  2018-12-04 00:03:11      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True          32  {'learning_rate': 0.1}  \n",
      "1     True          32  {'learning_rate': 0.1}  \n",
      "0.1 32 adam False\n",
      "0.870572\n",
      "0.883272\n",
      "2018-12-04 00:03:18\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.874502  1.706686   0.870572    1.564403  2018-12-04 00:03:18      adam   \n",
      "1  0.871417  1.808463   0.883272    1.484035  2018-12-04 00:03:18      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False          32  {'learning_rate': 0.1}  \n",
      "1    False          32  {'learning_rate': 0.1}  \n",
      "0.8739166666666667 0.8739166666666667\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.874502  1.706686   0.870572    1.564403  2018-12-04 00:03:18      adam   \n",
      "1  0.871417  1.808463   0.883272    1.484035  2018-12-04 00:03:18      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False          32  {'learning_rate': 0.1}  \n",
      "1    False          32  {'learning_rate': 0.1}  \n",
      "0.1 32 adagrad True\n",
      "0.914355\n",
      "0.921624\n",
      "2018-12-04 00:03:22\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.917596  0.302422   0.914355    0.308508  2018-12-04 00:03:22   adagrad   \n",
      "1  0.920780  0.287053   0.921624    0.284456  2018-12-04 00:03:22   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True          32  {'learning_rate': 0.1}  \n",
      "1     True          32  {'learning_rate': 0.1}  \n",
      "0.9219166666666667 0.9219166666666667\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.917596  0.302422   0.914355    0.308508  2018-12-04 00:03:22   adagrad   \n",
      "1  0.920780  0.287053   0.921624    0.284456  2018-12-04 00:03:22   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True          32  {'learning_rate': 0.1}  \n",
      "1     True          32  {'learning_rate': 0.1}  \n",
      "0.1 32 adagrad False\n",
      "0.917447\n",
      "0.923880\n",
      "2018-12-04 00:03:31\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.923666  0.275855   0.917447    0.292773  2018-12-04 00:03:31   adagrad   \n",
      "1  0.926354  0.269436   0.923880    0.272342  2018-12-04 00:03:31   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False          32  {'learning_rate': 0.1}  \n",
      "1    False          32  {'learning_rate': 0.1}  \n",
      "0.9285666666666667 0.9285666666666667\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.923666  0.275855   0.917447    0.292773  2018-12-04 00:03:31   adagrad   \n",
      "1  0.926354  0.269436   0.923880    0.272342  2018-12-04 00:03:31   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False          32  {'learning_rate': 0.1}  \n",
      "1    False          32  {'learning_rate': 0.1}  \n",
      "0.1 64 sgd True\n",
      "0.895988\n",
      "0.906751\n",
      "2018-12-04 00:03:36\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.903283  0.349065   0.895988    0.375098  2018-12-04 00:03:36       sgd   \n",
      "1  0.911491  0.317215   0.906751    0.329195  2018-12-04 00:03:36       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True          64  {'learning_rate': 0.1}  \n",
      "1     True          64  {'learning_rate': 0.1}  \n",
      "0.9065998134328358 0.9065998134328358\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.903283  0.349065   0.895988    0.375098  2018-12-04 00:03:36       sgd   \n",
      "1  0.911491  0.317215   0.906751    0.329195  2018-12-04 00:03:36       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True          64  {'learning_rate': 0.1}  \n",
      "1     True          64  {'learning_rate': 0.1}  \n",
      "0.1 64 sgd False\n",
      "0.896203\n",
      "0.907752\n",
      "2018-12-04 00:03:40\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.906349  0.335484   0.896203    0.379036  2018-12-04 00:03:40       sgd   \n",
      "1  0.913568  0.305846   0.907752    0.331979  2018-12-04 00:03:40       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False          64  {'learning_rate': 0.1}  \n",
      "1    False          64  {'learning_rate': 0.1}  \n",
      "0.9104810767590619 0.9104810767590619\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.906349  0.335484   0.896203    0.379036  2018-12-04 00:03:40       sgd   \n",
      "1  0.913568  0.305846   0.907752    0.331979  2018-12-04 00:03:40       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False          64  {'learning_rate': 0.1}  \n",
      "1    False          64  {'learning_rate': 0.1}  \n",
      "0.1 64 adam True\n",
      "0.863380\n",
      "0.875072\n",
      "2018-12-04 00:03:42\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.864221  1.645026   0.863380    1.620609  2018-12-04 00:03:42      adam   \n",
      "1  0.877472  1.638997   0.875072    1.644999  2018-12-04 00:03:42      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True          64  {'learning_rate': 0.1}  \n",
      "1     True          64  {'learning_rate': 0.1}  \n",
      "0.8747834488272921 0.8747834488272921\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.864221  1.645026   0.863380    1.620609  2018-12-04 00:03:42      adam   \n",
      "1  0.877472  1.638997   0.875072    1.644999  2018-12-04 00:03:42      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True          64  {'learning_rate': 0.1}  \n",
      "1     True          64  {'learning_rate': 0.1}  \n",
      "0.1 64 adam False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875608\n",
      "0.884904\n",
      "2018-12-04 00:03:47\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.879252  1.151672   0.875608    1.101710  2018-12-04 00:03:47      adam   \n",
      "1  0.885186  1.128571   0.884904    1.120773  2018-12-04 00:03:47      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False          64  {'learning_rate': 0.1}  \n",
      "1    False          64  {'learning_rate': 0.1}  \n",
      "0.8942730543710021 0.8942730543710021\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.879252  1.151672   0.875608    1.101710  2018-12-04 00:03:47      adam   \n",
      "1  0.885186  1.128571   0.884904    1.120773  2018-12-04 00:03:47      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False          64  {'learning_rate': 0.1}  \n",
      "1    False          64  {'learning_rate': 0.1}  \n",
      "0.1 64 adagrad True\n",
      "0.912078\n",
      "0.920552\n",
      "2018-12-04 00:03:49\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.916930  0.292891   0.912078    0.309960  2018-12-04 00:03:49   adagrad   \n",
      "1  0.922567  0.279846   0.920552    0.280814  2018-12-04 00:03:49   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True          64  {'learning_rate': 0.1}  \n",
      "1     True          64  {'learning_rate': 0.1}  \n",
      "0.9214418976545842 0.9214418976545842\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.916930  0.292891   0.912078    0.309960  2018-12-04 00:03:49   adagrad   \n",
      "1  0.922567  0.279846   0.920552    0.280814  2018-12-04 00:03:49   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True          64  {'learning_rate': 0.1}  \n",
      "1     True          64  {'learning_rate': 0.1}  \n",
      "0.1 64 adagrad False\n",
      "0.912150\n",
      "0.922018\n",
      "2018-12-04 00:03:55\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.924644  0.276562   0.912150    0.310205  2018-12-04 00:03:55   adagrad   \n",
      "1  0.927116  0.269755   0.922018    0.284974  2018-12-04 00:03:55   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False          64  {'learning_rate': 0.1}  \n",
      "1    False          64  {'learning_rate': 0.1}  \n",
      "0.9287213486140725 0.9287213486140725\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.924644  0.276562   0.912150    0.310205  2018-12-04 00:03:55   adagrad   \n",
      "1  0.927116  0.269755   0.922018    0.284974  2018-12-04 00:03:55   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False          64  {'learning_rate': 0.1}  \n",
      "1    False          64  {'learning_rate': 0.1}  \n",
      "0.1 128 sgd True\n",
      "0.864289\n",
      "0.895789\n",
      "2018-12-04 00:03:57\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.898926  0.383815   0.864289    0.573067  2018-12-04 00:03:57       sgd   \n",
      "1  0.908301  0.339376   0.895789    0.376088  2018-12-04 00:03:57       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True         128  {'learning_rate': 0.1}  \n",
      "1     True         128  {'learning_rate': 0.1}  \n",
      "0.9008528784648188 0.9008528784648188\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.898926  0.383815   0.864289    0.573067  2018-12-04 00:03:57       sgd   \n",
      "1  0.908301  0.339376   0.895789    0.376088  2018-12-04 00:03:57       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True         128  {'learning_rate': 0.1}  \n",
      "1     True         128  {'learning_rate': 0.1}  \n",
      "0.1 128 sgd False\n",
      "0.861541\n",
      "0.898221\n",
      "2018-12-04 00:04:00\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.902051  0.378587   0.861541    0.572466  2018-12-04 00:04:00       sgd   \n",
      "1  0.910352  0.333884   0.898221    0.373598  2018-12-04 00:04:00       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False         128  {'learning_rate': 0.1}  \n",
      "1    False         128  {'learning_rate': 0.1}  \n",
      "0.9035514392324094 0.9035514392324094\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.902051  0.378587   0.861541    0.572466  2018-12-04 00:04:00       sgd   \n",
      "1  0.910352  0.333884   0.898221    0.373598  2018-12-04 00:04:00       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False         128  {'learning_rate': 0.1}  \n",
      "1    False         128  {'learning_rate': 0.1}  \n",
      "0.1 128 adam True\n",
      "0.851429\n",
      "0.876816\n",
      "2018-12-04 00:04:02\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.854688  1.474513   0.851429    1.165309  2018-12-04 00:04:02      adam   \n",
      "1  0.850586  1.541806   0.876816    1.207249  2018-12-04 00:04:02      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True         128  {'learning_rate': 0.1}  \n",
      "1     True         128  {'learning_rate': 0.1}  \n",
      "0.8576925639658849 0.8576925639658849\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.854688  1.474513   0.851429    1.165309  2018-12-04 00:04:02      adam   \n",
      "1  0.850586  1.541806   0.876816    1.207249  2018-12-04 00:04:02      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True         128  {'learning_rate': 0.1}  \n",
      "1     True         128  {'learning_rate': 0.1}  \n",
      "0.1 128 adam False\n",
      "0.866588\n",
      "0.886394\n",
      "2018-12-04 00:04:05\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.900195  0.729189   0.866588    0.806751  2018-12-04 00:04:05      adam   \n",
      "1  0.900000  0.761890   0.886394    0.781035  2018-12-04 00:04:05      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False         128  {'learning_rate': 0.1}  \n",
      "1    False         128  {'learning_rate': 0.1}  \n",
      "0.9020355810234542 0.9020355810234542\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.900195  0.729189   0.866588    0.806751  2018-12-04 00:04:05      adam   \n",
      "1  0.900000  0.761890   0.886394    0.781035  2018-12-04 00:04:05      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False         128  {'learning_rate': 0.1}  \n",
      "1    False         128  {'learning_rate': 0.1}  \n",
      "0.1 128 adagrad True\n",
      "0.891558\n",
      "0.917194\n",
      "2018-12-04 00:04:06\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.915430  0.294875   0.891558    0.384906  2018-12-04 00:04:06   adagrad   \n",
      "1  0.922559  0.280728   0.917194    0.296730  2018-12-04 00:04:06   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True         128  {'learning_rate': 0.1}  \n",
      "1     True         128  {'learning_rate': 0.1}  \n",
      "0.9212086886993603 0.9212086886993603\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.915430  0.294875   0.891558    0.384906  2018-12-04 00:04:06   adagrad   \n",
      "1  0.922559  0.280728   0.917194    0.296730  2018-12-04 00:04:06   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True         128  {'learning_rate': 0.1}  \n",
      "1     True         128  {'learning_rate': 0.1}  \n",
      "0.1 128 adagrad False\n",
      "0.899220\n",
      "0.922008\n",
      "2018-12-04 00:04:10\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.921680  0.278431   0.899220    0.363448  2018-12-04 00:04:10   adagrad   \n",
      "1  0.925098  0.269104   0.922008    0.279987  2018-12-04 00:04:10   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False         128  {'learning_rate': 0.1}  \n",
      "1    False         128  {'learning_rate': 0.1}  \n",
      "0.9271388592750534 0.9271388592750534\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.921680  0.278431   0.899220    0.363448  2018-12-04 00:04:10   adagrad   \n",
      "1  0.925098  0.269104   0.922008    0.279987  2018-12-04 00:04:10   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False         128  {'learning_rate': 0.1}  \n",
      "1    False         128  {'learning_rate': 0.1}  \n",
      "0.1 256 sgd True\n",
      "0.847340\n",
      "0.886087\n",
      "2018-12-04 00:04:12\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.888672  0.447128   0.847340    0.708404  2018-12-04 00:04:12       sgd   \n",
      "1  0.901563  0.379389   0.886087    0.428211  2018-12-04 00:04:12       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True         256  {'learning_rate': 0.1}  \n",
      "1     True         256  {'learning_rate': 0.1}  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8925365691489362 0.8925365691489362\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.888672  0.447128   0.847340    0.708404  2018-12-04 00:04:12       sgd   \n",
      "1  0.901563  0.379389   0.886087    0.428211  2018-12-04 00:04:12       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True         256  {'learning_rate': 0.1}  \n",
      "1     True         256  {'learning_rate': 0.1}  \n",
      "0.1 256 sgd False\n",
      "0.843584\n",
      "0.887084\n",
      "2018-12-04 00:04:14\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.886719  0.445700   0.843584    0.711591  2018-12-04 00:04:14       sgd   \n",
      "1  0.899609  0.377672   0.887084    0.429247  2018-12-04 00:04:14       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False         256  {'learning_rate': 0.1}  \n",
      "1    False         256  {'learning_rate': 0.1}  \n",
      "0.8928025265957447 0.8928025265957447\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.886719  0.445700   0.843584    0.711591  2018-12-04 00:04:14       sgd   \n",
      "1  0.899609  0.377672   0.887084    0.429247  2018-12-04 00:04:14       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False         256  {'learning_rate': 0.1}  \n",
      "1    False         256  {'learning_rate': 0.1}  \n",
      "0.1 256 adam True\n",
      "0.856084\n",
      "0.880186\n",
      "2018-12-04 00:04:15\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.871875  0.945798   0.856084    0.916705  2018-12-04 00:04:15      adam   \n",
      "1  0.876953  1.017723   0.880186    0.926228  2018-12-04 00:04:15      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True         256  {'learning_rate': 0.1}  \n",
      "1     True         256  {'learning_rate': 0.1}  \n",
      "0.8740359042553192 0.8740359042553192\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.871875  0.945798   0.856084    0.916705  2018-12-04 00:04:15      adam   \n",
      "1  0.876953  1.017723   0.880186    0.926228  2018-12-04 00:04:15      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True         256  {'learning_rate': 0.1}  \n",
      "1     True         256  {'learning_rate': 0.1}  \n",
      "0.1 256 adam False\n",
      "0.873155\n",
      "0.889312\n",
      "2018-12-04 00:04:17\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.911133  0.483740   0.873155    0.617526  2018-12-04 00:04:17      adam   \n",
      "1  0.916602  0.507391   0.889312    0.575570  2018-12-04 00:04:17      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False         256  {'learning_rate': 0.1}  \n",
      "1    False         256  {'learning_rate': 0.1}  \n",
      "0.9149434840425532 0.9149434840425532\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.911133  0.483740   0.873155    0.617526  2018-12-04 00:04:17      adam   \n",
      "1  0.916602  0.507391   0.889312    0.575570  2018-12-04 00:04:17      adam   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False         256  {'learning_rate': 0.1}  \n",
      "1    False         256  {'learning_rate': 0.1}  \n",
      "0.1 256 adagrad True\n",
      "0.881998\n",
      "0.914960\n",
      "2018-12-04 00:04:19\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.908691  0.309734   0.881998    0.421163  2018-12-04 00:04:19   adagrad   \n",
      "1  0.914551  0.295749   0.914960    0.301945  2018-12-04 00:04:19   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True         256  {'learning_rate': 0.1}  \n",
      "1     True         256  {'learning_rate': 0.1}  \n",
      "0.9173703457446809 0.9173703457446809\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.908691  0.309734   0.881998    0.421163  2018-12-04 00:04:19   adagrad   \n",
      "1  0.914551  0.295749   0.914960    0.301945  2018-12-04 00:04:19   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0     True         256  {'learning_rate': 0.1}  \n",
      "1     True         256  {'learning_rate': 0.1}  \n",
      "0.1 256 adagrad False\n",
      "0.890908\n",
      "0.920113\n",
      "2018-12-04 00:04:21\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.920801  0.282543   0.890908    0.397692  2018-12-04 00:04:21   adagrad   \n",
      "1  0.925488  0.271863   0.920113    0.285348  2018-12-04 00:04:21   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False         256  {'learning_rate': 0.1}  \n",
      "1    False         256  {'learning_rate': 0.1}  \n",
      "0.925 0.925\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.920801  0.282543   0.890908    0.397692  2018-12-04 00:04:21   adagrad   \n",
      "1  0.925488  0.271863   0.920113    0.285348  2018-12-04 00:04:21   adagrad   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  \n",
      "0    False         256  {'learning_rate': 0.1}  \n",
      "1    False         256  {'learning_rate': 0.1}  \n",
      "0.2 8 sgd True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method NDArrayBase.__del__ of \n",
      "[[[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]]\n",
      "<NDArray 8x1x28x28 @cpu(0)>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\luiz\\AppData\\Roaming\\Python\\Python36\\site-packages\\mxnet\\_ctypes\\ndarray.py\", line 51, in __del__\n",
      "    check_call(_LIB.MXNDArrayFree(self.handle))\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from svrg_optimization.svrg_module import SVRGModule\n",
    "except ModuleNotFoundError:\n",
    "    download_svrg_module()\n",
    "    from svrg_optimization.svrg_module import SVRGModule\n",
    "\n",
    "run = True\n",
    "\n",
    "# define hyperparameters\n",
    "#lr = 0.2\n",
    "#batch_size = 32\n",
    "epochs = 10\n",
    "#step = 300\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "if run:\n",
    "    for _lr in range(1, 10):\n",
    "        lr = _lr/10\n",
    "        for _batch_size in range(3, 9):\n",
    "            batch_size = 2**_batch_size\n",
    "            for optimizer in ['sgd', 'adam', 'adagrad']:\n",
    "                for is_svrg in [True, False]:\n",
    "                    ctx = mx.cpu()\n",
    "                    print(lr, batch_size, optimizer, is_svrg)\n",
    "                    logreg = LogisticRegression(\n",
    "                        dataset='MNIST', optimizer=optimizer, epochs=epochs, context=ctx, batch_size=batch_size, is_svrg=is_svrg,\n",
    "                        hyperparams={'learning_rate': lr})\n",
    "                    logreg.run_logistic(2)\n",
    "                    \n",
    "                    df = pd.concat([df, logreg.df_out])\n",
    "                \n",
    "                df.to_pickle('output.pkl')    \n",
    "                    #print(lr_svrg_sgd.df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_svrg_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.889266\n",
      "0.897955\n",
      "2018-12-04 00:21:46\n",
      "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
      "0  0.904061  0.348061   0.889266    0.381538  2018-12-04 00:21:46       sgd   \n",
      "1  0.906947  0.348599   0.897955    0.361218  2018-12-04 00:21:46       sgd   \n",
      "\n",
      "   is_svrg  batch_size             hyperparams  epochs  \n",
      "0     True          32  {'learning_rate': 0.2}       1  \n",
      "1     True          32  {'learning_rate': 0.2}       2  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        from svrg_optimization.svrg_module import SVRGModule\n",
    "    except ModuleNotFoundError:\n",
    "        download_svrg_module()\n",
    "        from svrg_optimization.svrg_module import SVRGModule\n",
    "\n",
    "    run = True\n",
    "\n",
    "    # define hyperparameters\n",
    "    lr = 0.2\n",
    "    batch_size = 32\n",
    "    epochs = 2\n",
    "    step = 300\n",
    "\n",
    "    if run:\n",
    "        ctx = mx.cpu()\n",
    "        lr_svrg_sgd = LogisticRegression(\n",
    "            dataset='MNIST', optimizer='sgd', epochs=epochs, context=ctx, batch_size=batch_size, is_svrg=True,\n",
    "            hyperparams={'learning_rate': 0.2})\n",
    "        lr_svrg_sgd.run_logistic(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.concat([dd, lr_svrg_sgd.df_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.to_pickle('out.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>is_svrg</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.904061</td>\n",
       "      <td>0.348061</td>\n",
       "      <td>0.889266</td>\n",
       "      <td>0.381538</td>\n",
       "      <td>2018-12-04 00:21:46</td>\n",
       "      <td>sgd</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>{'learning_rate': 0.2}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.906947</td>\n",
       "      <td>0.348599</td>\n",
       "      <td>0.897955</td>\n",
       "      <td>0.361218</td>\n",
       "      <td>2018-12-04 00:21:46</td>\n",
       "      <td>sgd</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>{'learning_rate': 0.2}</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.904061</td>\n",
       "      <td>0.348061</td>\n",
       "      <td>0.889266</td>\n",
       "      <td>0.381538</td>\n",
       "      <td>2018-12-04 00:21:46</td>\n",
       "      <td>sgd</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>{'learning_rate': 0.2}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.906947</td>\n",
       "      <td>0.348599</td>\n",
       "      <td>0.897955</td>\n",
       "      <td>0.361218</td>\n",
       "      <td>2018-12-04 00:21:46</td>\n",
       "      <td>sgd</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>{'learning_rate': 0.2}</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
       "0  0.904061  0.348061   0.889266    0.381538  2018-12-04 00:21:46       sgd   \n",
       "1  0.906947  0.348599   0.897955    0.361218  2018-12-04 00:21:46       sgd   \n",
       "0  0.904061  0.348061   0.889266    0.381538  2018-12-04 00:21:46       sgd   \n",
       "1  0.906947  0.348599   0.897955    0.361218  2018-12-04 00:21:46       sgd   \n",
       "\n",
       "   is_svrg  batch_size             hyperparams  epochs  \n",
       "0     True          32  {'learning_rate': 0.2}       1  \n",
       "1     True          32  {'learning_rate': 0.2}       2  \n",
       "0     True          32  {'learning_rate': 0.2}       1  \n",
       "1     True          32  {'learning_rate': 0.2}       2  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle('out.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_svrg_sgd.df_out['timestamp'] = pd.Series('aa', index=lr_svrg_sgd.df_out.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>is_svrg</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>hyperparams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.894307</td>\n",
       "      <td>0.367086</td>\n",
       "      <td>0.893549</td>\n",
       "      <td>0.371103</td>\n",
       "      <td>2018-12-03 23:53:46</td>\n",
       "      <td>sgd</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>{'learning_rate': 0.2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.896696</td>\n",
       "      <td>0.365647</td>\n",
       "      <td>0.902072</td>\n",
       "      <td>0.352716</td>\n",
       "      <td>2018-12-03 23:53:46</td>\n",
       "      <td>sgd</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "      <td>{'learning_rate': 0.2}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_acc  val_loss  train_acc  train_loss            timestamp optimizer  \\\n",
       "0  0.894307  0.367086   0.893549    0.371103  2018-12-03 23:53:46       sgd   \n",
       "1  0.896696  0.365647   0.902072    0.352716  2018-12-03 23:53:46       sgd   \n",
       "\n",
       "   is_svrg  batch_size             hyperparams  \n",
       "0     True          32  {'learning_rate': 0.2}  \n",
       "1     True          32  {'learning_rate': 0.2}  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_svrg_sgd.df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.2}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_svrg_sgd.hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE\n",
    "# python plot_log.py --network VGGNet --dataset ImageNet\n",
    "\n",
    "# import the necessary packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the argument parse and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-n\", \"--network\", required=True,\n",
    "    help=\"name of network\")\n",
    "ap.add_argument(\"-d\", \"--dataset\", required=True,\n",
    "    help=\"name of dataset\")\n",
    "args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.888954\n",
      "0.893549\n",
      "0.919786\n"
     ]
    }
   ],
   "source": [
    "# define the paths to the training logs\n",
    "logs = [\n",
    "    (0, \"example2.log\")\n",
    "]\n",
    "\n",
    "# initialize the list of train rank-1 and rank-5 accuracies, along\n",
    "# with the training loss\n",
    "(trainRank1, trainRank5, trainLoss) = ([], [], [])\n",
    "\n",
    "# initialize the list of validation rank-1 and rank-5 accuracies,\n",
    "# along with the validation loss\n",
    "(valRank1, valRank5, valLoss) = ([], [], [])\n",
    "\n",
    "# loop over the training logs\n",
    "for (i, (endEpoch, p)) in enumerate(logs):\n",
    "    # load the contents of the log file, then initialize the batch\n",
    "    # lists for the training and validation data\n",
    "    rows = open(p).read().strip()\n",
    "    (bTrainRank1, bTrainRank5, bTrainLoss) = ([], [], [])\n",
    "    (bValRank1, bValRank5, bValLoss) = ([], [], [])\n",
    "\n",
    "    # grab the set of training epochs\n",
    "    epochs = set(re.findall(r'Epoch\\[(\\d+)\\]', rows))\n",
    "    epochs = sorted([int(e) for e in epochs])\n",
    "\n",
    "    # loop over the epochs\n",
    "    for e in epochs:\n",
    "        # find all rank-1 accuracies, rank-5 accuracies, and loss\n",
    "        # values, then take the final entry in the list for each\n",
    "        s = r'Epoch\\[' + str(e) + '\\].*accuracy=([0]*\\.?[0-9]+)'\n",
    "        \n",
    "        rank1 = re.findall(s, rows)[-2]\n",
    "        print(rank1)\n",
    "        #s = r'Epoch\\[' + str(e) + '\\].*top_k_accuracy_5=([0]*\\.?[0-9]+)'\n",
    "        #rank5 = re.findall(s, rows)[-2]\n",
    "        s = r'Epoch\\[' + str(e) + '\\].*cross-entropy=([0-9]*\\.?[0-9]+)'\n",
    "        loss = re.findall(s, rows)[-2]\n",
    "\n",
    "        # update the batch training lists\n",
    "        bTrainRank1.append(float(rank1))\n",
    "        #bTrainRank5.append(float(rank5))\n",
    "        bTrainLoss.append(float(loss))\n",
    "    \n",
    "    # extract the validation rank-1 and rank-5 accuracies for each\n",
    "    # epoch, followed by the loss\n",
    "    bValRank1 = re.findall(r'Validation-accuracy=(.*)', rows)\n",
    "    #bValRank5 = re.findall(r'Validation-top_k_accuracy_5=(.*)', rows)\n",
    "    bValLoss = re.findall(r'Validation-cross-entropy=(.*)', rows)\n",
    "\n",
    "    # convert the validation rank-1, rank-5, and loss lists to floats\n",
    "    bValRank1 = [float(x) for x in bValRank1]\n",
    "    #bValRank5 = [float(x) for x in bValRank5]\n",
    "    bValLoss = [float(x) for x in bValLoss]\n",
    "\n",
    "    # check to see if we are examining a log file other than the\n",
    "    # first one, and if so, use the number of the final epoch in\n",
    "    # the log file as our slice index\n",
    "    if i > 0 and endEpoch is not None:\n",
    "        trainEnd = endEpoch - logs[i - 1][0]\n",
    "        valEnd = endEpoch - logs[i - 1][0]\n",
    "\n",
    "    # otherwise, this is the first epoch so no subtraction needs\n",
    "    # to be done\n",
    "    else:\n",
    "        trainEnd = endEpoch\n",
    "        valEnd = endEpoch\n",
    "\n",
    "    # update the training lists\n",
    "    trainRank1.extend(bTrainRank1[0:trainEnd])\n",
    "    #trainRank5.extend(bTrainRank5[0:trainEnd])\n",
    "    trainLoss.extend(bTrainLoss[0:trainEnd])\n",
    "\n",
    "    # update the validation lists\n",
    "    valRank1.extend(bValRank1[0:valEnd])\n",
    "    #valRank5.extend(bValRank5[0:valEnd])\n",
    "    valLoss.extend(bValLoss[0:valEnd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.388628, 0.372752, 0.292123]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bTrainLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracies\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, len(trainRank1)), trainRank1,\n",
    "    label=\"train_rank1\")\n",
    "plt.plot(np.arange(0, len(trainRank5)), trainRank5,\n",
    "    label=\"train_rank5\")\n",
    "plt.plot(np.arange(0, len(valRank1)), valRank1,\n",
    "    label=\"val_rank1\")\n",
    "plt.plot(np.arange(0, len(valRank5)), valRank5,\n",
    "    label=\"val_rank5\")\n",
    "plt.title(\"{}: rank-1 and rank-5 accuracy on {}\".format(\n",
    "    args[\"network\"], args[\"dataset\"]))\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# plot the losses\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, len(trainLoss)), trainLoss,\n",
    "    label=\"train_loss\")\n",
    "plt.plot(np.arange(0, len(valLoss)), valLoss,\n",
    "    label=\"val_loss\")\n",
    "plt.title(\"{}: cross-entropy loss on {}\".format(args[\"network\"],\n",
    "    args[\"dataset\"]))\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mxnet_gpu)",
   "language": "python",
   "name": "mxnet_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
